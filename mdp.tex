\section{The Control Improvisation Problem for MDPs}
\label{sec:mdps}

We present an algorithm for the control improvisation problem for
MDPs, which in the next section, will serve as a subroutine for an algorithm
on SGs. To start, recall that an MDP is a stochastic game with no action choices for the environment, i.e., the environment is purely stochastic. 
We want to instantiate the approximation scheme from the previous section. In particular, that means that we need to find points on the Pareto curve $\pareto{\solutions}$ and thus incrementally build up $\pareto{} \subseteq \pareto{\solutions}$. 

The
key ingredient to find points on the Pareto-curve is to rephrase the trade-off between randomization and
performance as a degree in rationality $\rat$ of the
policy. Intuitively, a rationality of $\rat = \infty$ means that we
focus completely on the performance criterion, and rationality $\rat =
0$. We can then reuse ideas from maximum entropy inference from
specifications~\cite{DBLP:conf/cav/Vazquez-Chanlatte20}.

\subsection{Rationality}

\noindent
In context of MDPs, the maximum causal entropy policy consistent with
an expected reward (here performance, $\scp$) is given by a
smooth variant of the Bellman equations~\cite{mceThesis}. Namely, let
$\smoothmax{}$ denote the log-sum-exp operator, i.e., $\smoothmax(X)
\eqdef \log \left( \sum_{x\in X} e^x \right)$. For each rationality
$\rat \in [0, \infty)$, we define a policy,
 \begin{align}
   &\sched_\rat(s\mid \act) \eqdef \exp( Q_\rat(s,\act) - V_\rat(s))  \\
   & V_\rat(s) \eqdef  \begin{cases}
     \lambda  \cdot \indicator{s = \target} & \text{if }s \in \{ \target, \sink \},\\
     \smoothmax_{\act \in \EnAct(s)}{  Q_\rat(s,\act) } & \text{otherwise.}
   \end{cases}\label{eq:mdp:v}\\ 
	& Q_\rat(s, \act) \eqdef \sum_{s'} P(s,\act,s') \cdot V_\rat(s').
 \end{align}

To ease notation, we denote $x_\rat \eqdef x_{\sched_\rat},
\scp_\rat \eqdef \scp_{\sched_\rat}, \rndp_\rat \eqdef
\rndp_{\sched_\rat}$. 
We remark that for $\lambda \rightarrow \infty$, this variant of the Bellman-equations coincide with the standard Bellman equations~\cite{DBLP:books/wi/Puterman94}.
As previously alluded, the key property
is that $\sched_\rat$ is the \emph{unique} maximum causal entropy policy
such that $\Pr(\varphi) = \scp_\rat$~\cite{mceThesis}.

Intuitively, as $\rat$ approaches $0$, $\sched_\rat$ approaches the
uniform distribution over \emph{all available actions}. Note that this
policy maximizes (causal) entropy, and thus $\rndopt = \rndp_0$.
Similarly, as $\rat$ approaches $\infty$, $\sched_\rat$ selects (uniformly) from
actions \emph{that maximize performance}. Thus, $\scopt = \scp_\infty$.
Furthermore, we obtain from the Bellman-equations:
\begin{proposition}
  $\scp_\rat$ is  smoothly (and strictly) increasing in $\rat$ and $\rndp_\rat$
  is smoothly (and strictly) decreasing in $\rat$.
\end{proposition}
Thus, this
family spans the Pareto-Front, $\pareto{\solutions}$.
In terms of $\solfuncp$, we 
can define $\epsilon_\rat \eqdef \frac{\scp_\rat - \scp_0}{\scp_\infty} + \scp_0, \delta_\rat \eqdef \frac{\rndp_\rat - \rndp_\infty}{\rndp_0} + \rndp_{\infty}$. Then, because
$\sched_\rat$ maximizes randomness given a target performance, one derives:
\begin{equation}
  \solfuncp\left(\delta_\rat\right) = \epsilon_\rat.
\end{equation}

The key idea now is to instantiate the approximation scheme for the Pareto-front by varying $\rat$.\footnote{Assuming $\scopt, \rndopt \neq 0$ (which would
otherwise yield trivial $\solutions$ and $\pareto{\solutions}$)}.
In particular, we construct $\pareto{} = \{ x_\rat \mid \rat \in \{ \rat_1, \rat_2, \hdots \} \}$ until $\pareto{}$ contains a witness to either satisfiability or unsatisfiability. 
In the remainder of this section, we improve upon randomly selection values for $\rat$.

\begin{remark}
Given a witness(pair) to satisfiability, it is easy to extract the corresponding improvisor. Let $x_1,x_2$ be a witness-pair to satisfiability,  induced by $\sched_{\rat_1}$ and $\sched_{\rat_2}$ such that $\langle\scthreshold, \randomness\rangle\preceq q\cdot x_1 + \bar{q}\cdot x_2$, then the policy described by  	
\begin{equation}\label{eq:4}
  \sigma^*_\pOne(\act \mid s) \eqdef q\cdot \sched_{\rat_1}(\act \mid s) + \bar{q} \cdot \sched_{\rat_2}(\act  \mid s)
\end{equation}
is an improvisor solving the ERCI problem.
\end{remark}



%
%and by varying $\rat$ we can explore the Pareto-front. First observe the following easily verified proposition.
%\begin{proposition}
%  $\scp_\rat$ is smoothly and (strictly) monotonically increasing in $\rat$ and $\rndp_\rat$
%  is smoothly (strictly) monotonically decreasing in $\rat$.
%\end{proposition}

\subsection{Targeted Pareto-exploration}
The key ingredient to improve upon arbitrarily selecting $\rat_1, \hdots \rat_i$ is to exploit additional structure of the rationality.  
%The key algorithmic idea is thus to strategically evaluate a sequence
%of rationality coefficients to yield (input, output) pairs for
%$\solfuncp$. Due to convexity, the convex hull this sequence of
%rationality-indexed points (and the origin) gradually refines a
%polygonal approximation of $\solutions$, and thus the Pareto
%Front. This approximation, $\hat{\solutions}$, is refined until
%either:
%\begin{enumerate}
%\item $\langle \scthreshold, \randomness \rangle \in \hat{\solutions}$ proving
%  $\langle \scthreshold, \randomness \rangle \in \solutions$.
%\item A $\rat$ is found such that
%  $x_{\rat} \prec \langle \scthreshold, \randomness \rangle$, proving
%  $\langle \scthreshold, \randomness \rangle \notin \solutions$.
%\end{enumerate}
%
%
%Next, to extract an improviser, observe that because
%$\hat{\solutions}$ is a convex polygon, if $\langle \scthreshold,
%\randomness \rangle \in \hat{S}$, then there must two corners of
%$\hat{\solutions}$ indexed by $\rat_1$ and $\rat_2$, that form a
%triangle with $(0, 0)$ containing $\langle \scthreshold, \randomness
%\rangle$. Thus, as in the convexity proof, there must be a convex
%combination of $q\cdot x_{\rat_1} + \bar{q}\cdot x_{\rat_2}$ that
%dominates $\langle \scthreshold, \randomness \rangle$. Therefore, the
%following policy solves the ERCI instance:

%\mypara{Approximation Sequence} 
%The final algorithmic question for
%MDPs is then: what order should one evaluate rationality coefficients.
We propose a three staged sequence: (i) Compute $x_\rat$ for the end
points $\rat \in \{0, \infty\}$.  (ii) Double $\rat$ until $h_\rat \leq
\randomness$, yielding $\rat_1\ldots \rat_j$, where $\rat_1 = 1$.
(iii) Binary search for $\rat \in [\rat_{j-1}, \rat_{j}]$. We illustrate the idea in Fig.~\ref{fig:geom:doubling}.

The algorithm terminates almost surely, that is: 
the algorithm halts if $\langle \scthreshold, \randomness \rangle$ is
not on $\pareto{\solutions}$ (or if we happen to exactly hit $\langle \scthreshold, \randomness\rangle$ by selecting some rationality $\rat$).
As the Pareto Front has
measure 0, we argue that not halting is thus merely a technical concern, as a
small perturbation to the ERCI instance (i.e. a \emph{smoothed
analysis}~\cite{SmoothedAnalysis}) on $\sg$ admits decidability. 
\begin{mdframed}
  Our approximation scheme yields a semi-decision process which halts
  iff either (a) $\langle \scthreshold, \randomness \rangle$ is
  bounded away from $\pareto{\solutions}$ \emph{or} (b)
  $\langle \scthreshold, \randomness \rangle$ is dominated by
  $x_{\rat_i}$.
\end{mdframed}
Next, observe that given a maximum resolution, $\kappa$,\sj{What is $\kappa$} in
rationality, this approximation scheme becomes linear in the MDP size
and logarithmic in the final rationality coefficient $\rat_*$ and the
resolution $\kappa$, i.e., the run-time is,
\begin{equation}
  O\bigg(\hspace{-1.4em}\underbrace{|\sg|}_{\text{Evaluate Point $x_\rat$}}\hspace{-1.4em}\cdot\overbrace{\log(\rat_*)}^{\text{Doubling Phase}}\cdot\underbrace{\log(\nicefrac{1}{\kappa})}_{\text{Binary Search}}\bigg)
\end{equation}
Finally, before generalizing to stochastic games, we observe that in
practice, $\sched_{100} \approx \sched_\infty$, and one can often take
$\rat_* \leq 100$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
