\section{The Control Improvisation Problem for MDPs}
\label{sec:mdps}

We present an algorithm for the control improvisation problem for
MDPs, which in the next section, will serve as a subroutine for an
algorithm on SGs. Our goal shall be to instantiate the approximation
scheme from the previous section. In particular, we seek
to find points on the Pareto curve $\pareto{\solutions}$ and
incrementally build up $\pareto{} \subseteq \pareto{\solutions}$.
\subsection{Rationality}
To start, recall that an MDP is a stochastic game with no action
choices for the environment, i.e., the environment is purely
stochastic and the only degree of freedom is $\pOne$'s policy.  The
key idea for finding points on the Pareto-curve is to rephrase the
trade-off between randomization and performance as a degree in
rationality $\rat$ of the policy.  Formally, the rationality
corresponds to the following scalarization of our multi-objective
problem~\cite{DBLP:journals/corr/abs-1805-00909},
\begin{equation}
  \label{eq:scalarization}
  J_\rat(\sched) \eqdef \Big\langle 1, \rat\Big\rangle \cdot \Big\langle\rndp_\sched, \scp_\sched\Big\rangle.
\end{equation}
In context of MDPs, the \textbf{unique} ($\pOne$-)policy that
optimizes~\eqref{eq:scalarization} is given by a smooth variant of the
Bellman equations~\cite{mceThesis, DBLP:conf/cav/Vazquez-Chanlatte20}. Namely, let $\smoothmax{}$ denote
the log-sum-exp operator, i.e.,
$\smoothmax(X) \eqdef \log \left( \sum_{x\in X} e^x \right)$. For each
rationality $\rat \in [0, \infty)$, we define a policy $\sched_\rat$
-- using $s = \last{\path}$ -- as follows:
 \begin{align}
   &\sched_\rat(\act \mid s) \eqdef \exp( Q_\rat(s,\act) - V_\rat(s))  \label{eq:mdp:first}\\
   & V_\rat(s) \eqdef  \begin{cases}
     \lambda  \cdot \indicator{s = \target} & \text{if }s \in \{ \target, \sink \},\\
     \smoothmax_{\act \in \EnAct(s)}{  Q_\rat(s,\act) } & \text{otherwise.}
   \end{cases}\label{eq:mdp:v}\\ 
	& Q_\rat(s, \act) \eqdef \sum_{s'} P(s,\act,s') \cdot V_\rat(s').\label{eq:mdp:last}
 \end{align}
To ease notation, we denote $x_\rat \eqdef x_{\sched_\rat},
\scp_\rat \eqdef \scp_{\sched_\rat}, \rndp_\rat \eqdef
\rndp_{\sched_\rat}$. 
 % As previously alluded at the start of the subsection, the key property
% is that $\sched_\rat$ is the \emph{unique} maximum causal entropy policy
% such that $\Pr(\varphi) = \scp_\rat$~\cite{mceThesis}.
Intuitively, as $\rat \rightarrow 0$, $\sched_\rat$ approaches the
uniform distribution over \emph{all available actions}. Note that this
policy maximizes (causal) entropy, and thus $\rndopt = \rndp_0$.  As
$\lambda \rightarrow \infty$, this variant of the Bellman equations
coincides with the standard Bellman
equations~\cite{DBLP:books/wi/Puterman94}, where $\sched_\rat$ selects
(uniformly) from actions \emph{that maximize performance}.
Furthermore, the monotonicity and smoothness of the above Bellman
equations yields the following proposition.
\begin{proposition}
  $\scp_\rat$ is  continuously (and strictly) increasing in $\rat$ and $\rndp_\rat$
  is smoothly (and strictly) decreasing in $\rat$.
\end{proposition}
\noindent In terms of $\solfuncp$, we can define:
\begin{equation}
  \epsilon_\rat \eqdef \frac{\scp_\rat - \scp_0}{\scp_\infty} + \scp_0
  \hspace{1em}\text{and}\hspace{1em}
\delta_\rat \eqdef \frac{\rndp_\rat - \rndp_\infty}{\rndp_0} +
\rndp_{\infty}. 
\end{equation}

Then, because $\sched_\rat$ maximizes randomness
given a target performance, one derives:
\begin{equation}
  \solfuncp\left(\delta_\rat\right) = \epsilon_\rat.
\end{equation}

What remains is to instantiate the approximation scheme for the Pareto
front by varying the optimization direction
$\langle \rat, 1\rangle$.\footnote{Assuming $\scopt, \rndopt \neq 0$
  (which would otherwise yield trivial $\solutions$ and
  $\pareto{\solutions}$)}  In particular, we construct
$\pareto{} = \{ x_\rat \mid \rat \in \{ \rat_1, \rat_2, \hdots \} \}$
until $\pareto{}$ contains a witness to either realizability or
unrealizability of the ERCI instance. We notice that the scalarization in \eqref{eq:scalarization} means that we may additionally exploit witnesses to unrealizability as outlined in Remark~\ref{rem:scalarwitnesses}. In the remainder of this
section, we improve upon randomly selecting values for $\rat$.



%
%and by varying $\rat$ we can explore the Pareto front. First observe the following easily verified proposition.
%\begin{proposition}
%  $\scp_\rat$ is smoothly and (strictly) monotonically increasing in $\rat$ and $\rndp_\rat$
%  is smoothly (strictly) monotonically decreasing in $\rat$.
%\end{proposition}

\subsection{Targeted Pareto-exploration}
The key ingredient to improve upon arbitrarily selecting $\rat_1, \hdots \rat_i$ is to exploit additional structure of the rationality.  
%The key algorithmic idea is thus to strategically evaluate a sequence
%of rationality coefficients to yield (input, output) pairs for
%$\solfuncp$. Due to convexity, the convex hull this sequence of
%rationality-indexed points (and the origin) gradually refines a
%polygonal approximation of $\solutions$, and thus the Pareto
%Front. This approximation, $\hat{\solutions}$, is refined until
%either:
%\begin{enumerate}
%\item $\langle \scthreshold, \randomness \rangle \in \hat{\solutions}$ proving
%  $\langle \scthreshold, \randomness \rangle \in \solutions$.
%\item A $\rat$ is found such that
%  $x_{\rat} \prec \langle \scthreshold, \randomness \rangle$, proving
%  $\langle \scthreshold, \randomness \rangle \notin \solutions$.
%\end{enumerate}
%
%
%Next, to extract an improviser, observe that because
%$\hat{\solutions}$ is a convex polygon, if $\langle \scthreshold,
%\randomness \rangle \in \hat{S}$, then there must two corners of
%$\hat{\solutions}$ indexed by $\rat_1$ and $\rat_2$, that form a
%triangle with $(0, 0)$ containing $\langle \scthreshold, \randomness
%\rangle$. Thus, as in the convexity proof, there must be a convex
%combination of $q\cdot x_{\rat_1} + \bar{q}\cdot x_{\rat_2}$ that
%dominates $\langle \scthreshold, \randomness \rangle$. Therefore, the
%following policy solves the ERCI instance:

%\mypara{Approximation Sequence} 
%The final algorithmic question for
%MDPs is then: what order should one evaluate rationality coefficients.
We propose a three staged sequence: (i) Compute $x_\rat$ for the end
points $\rat \in \{0, \infty\}$.  (ii) Double $\rat$ (starting at $\lambda=1$) until $h_\rat \leq
\randomness$, yielding $\rat_1\ldots \rat_j$.
(iii) Binary search for $\rat \in [\rat_{j-1}, \rat_{j}]$. We illustrate the idea in Fig.~\ref{fig:geom:doubling}.

The algorithm terminates almost surely, that is: 
the algorithm halts if $\langle \scthreshold, \randomness \rangle$ is
not on $\pareto{\solutions}$ (or if we happen to exactly hit $\langle \scthreshold, \randomness\rangle$ by selecting some rationality $\rat$).
As the Pareto front has
measure 0, we argue that not halting is thus merely a technical concern, as a
small perturbation to the ERCI instance (i.e. a \emph{smoothed
analysis}~\cite{SmoothedAnalysis}) on $\sg$ admits decidability. 
\begin{mdframed}
  Our approximation scheme yields a semi-decision process which halts
  iff either (a) $\langle \scthreshold, \randomness \rangle$ is
  bounded away from $\pareto{\solutions}$ \emph{or} (b)
  $\langle \scthreshold, \randomness \rangle$ is dominated by
  $x_{\rat_i}$.
\end{mdframed}
Next, observe that if we terminate the binary search when the search
region is smaller than $\Delta$, this approximation scheme becomes linear in the MDP size
and logarithmic in the final rationality, $\rat_*$, and the
resolution, $\Delta$, i.e., the run-time is,
\begin{equation}
  \mathcal{O}\Big(~\hspace{-1.4em}\underbrace{|\sg|}_{\text{Evaluate $x_\rat$}}\hspace{-1.4em}~\cdot\overbrace{\log(\rat_*)}^{\text{Doubling Phase}}\cdot\underbrace{\log(\nicefrac{1}{\Delta})}_{\text{Binary Search}}\Big)
\end{equation}
Finally, before generalizing to stochastic games, we observe that in
practice, $\rat = 100$ yields a nearly optimal policy, and thus one
can often assume $\rat_* \leq 100$ in our run-time analysis.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
