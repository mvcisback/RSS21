\section{The Control Improvisation Problem for MDPs}
\label{sec:mdps}

We present an algorithm for the control improvisation problem for
MDPs, which in the next section, will serve as basis for an algorithm
on SGs. To start, recall that an MDP is a stochastic game with a The
key idea is to rephrase the tradeoff between randomisation and
performance as a degree in rationality $\rat$ of the
policy. Intuitively, a rationality of $\rat = \infty$ means that we
focus completely on the performance criterion, and rationality $\rat =
0$. We can then reuse ideas from maximum entropy inference from
specifications~\cite{DBLP:conf/cav/Vazquez-Chanlatte20}.

\subsection{Rationality}

\noindent
In context of MDPs, the maximum causal entropy policy consistent with
an expected reward (here performance, $\scthreshold$) is given by a
smooth variant of the Bellman equations~\cite{mceThesis}. Namely, let
$\smoothmax{}$ denote the log-sum-exp operator, i.e., $\smoothmax(X)
\eqdef \log \left( \sum_{x\in X} e^x \right)$. For each rationality
$\rat \in [0, \infty)$, we define a policy,
 \begin{align}
   &\sched_\rat(s\mid \act) \eqdef \exp( Q_\rat(s,\act) - V_\rat(s))  \\
   & V_\rat(s) \eqdef  \begin{cases}
     \lambda  \cdot \indicator{s = \target} & \text{if }s \in \{ \target, \sink \},\\
     \smoothmax_{\act \in \EnAct(s)}{  Q_\rat(s,\act) } & \text{otherwise.}
   \end{cases}\\ 
	& Q_\rat(s, \act) \eqdef \sum_{s'} P(s,\act,s') \cdot V_\rat(s').
 \end{align}
\sj{Need to put in enact}
To ease notation, we denote $x_\rat \colonequals x_{\sched_\rat},
\scp_\rat \colonequals \scp_{\sched_\rat}, \rndp_\rat \colonequals
\rndp_{\sched_\rat}$. As previously alluded, the key property
is that $\sched_\rat$ is the \emph{unique} maximum causal entropy policy
such that $\Pr(\varphi) = \scp_\rat$~\cite{DBLP:conf/cav/Vazquez-Chanlatte20}.

In terms of the machinery developed in the previous section, this
family serves to index the Pareto-Front, $\pareto{\solutions}$.  As a
consequence, we can use $\rat$ to explore the Pareto-front.  To see
this, first observe the following easily verified proposition.

\begin{proposition}
  $\scp_\rat$ is smoothly and (strictly) monotonically increasing in $\rat$ and $\rndp_\rat$
  is smoothly (strictly) monotonically decreasing in $\rat$.
\end{proposition}

Intuitively, as $\rat$ approaches $0$, $\sched_\rat$ approaches the
uniform distribution over \emph{all available actions}. Note that this
policy maximizes (causal) entropy, and thus $\rndopt = \rndp_0$.
Similarly, as $\rat$ approaches $\infty$, $\sched_\rat$ selects (uniformly) from
actions \emph{that maximize performance}. Thus, $\scopt = \scp_\infty$.

\subsection{Pareto-exploration}
As a consequence, we can use $\rat$ to explore the Pareto-front.
In particular, assuming $\scopt, \rndopt \neq 0$ (which would
otherwise yield trivial $\solutions$ and $\pareto{\solutions}$), one
can define $\epsilon_\rat \eqdef \nicefrac{\scp_\rat}{\scp_\infty}, \delta_\rat \eqdef \nicefrac{\rndp_\rat}{\rndp_0}$. Then, because
$\sched_\rat$ maximizes randomness given a target performance, one derives:
\begin{equation}
  \solfuncp\left(\delta_\rat\right) = \epsilon_\rat.
\end{equation}
The key algorithmic idea is thus to strategically evaluate a sequence
of rationality coefficients to yield (input, output) pairs for
$\solfuncp$. Due to convexity, the convex hull this sequence of
rationality-indexed points (and the origin) gradually refines a
polygonal approximation of $\solutions$, and thus the Pareto
Front. This approximation, $\hat{\solutions}$, is refined until
either:
\begin{enumerate}
\item $\langle \scthreshold, \randomness \rangle \in \hat{\solutions}$ proving
  $\langle \scthreshold, \randomness \rangle \in \solutions$.
\item A $\rat$ is found such that
  $x_{\rat} \prec \langle \scthreshold, \randomness \rangle$, proving
  $\langle \scthreshold, \randomness \rangle \notin \solutions$.
\end{enumerate}
Next, to extract an improviser, observe that because
$\hat{\solutions}$ is a convex polygon, if $\langle \scthreshold,
\randomness \rangle \in \hat{S}$, then there must two corners of
$\hat{\solutions}$ indexed by $\rat_1$ and $\rat_2$, that form a
triangle with $(0, 0)$ containing $\langle \scthreshold, \randomness
\rangle$. Thus, as in the convexity proof, there must be a convex
combination of $q\cdot x_{\rat_1} + \bar{q}\cdot x_{\rat_2}$ that
dominates $\langle \scthreshold, \randomness \rangle$. Therefore, the
following policy solves the ERCI instance:
\begin{equation}\label{eq:4}
  \sigma^*_\pOne(a\mid s) \eqdef q\cdot \sched_{\rat_1}(a\mid s) + \bar{q} \cdot \sched_{\rat_2}(a \mid s)
\end{equation}

\mypara{Approximation Sequence} The final algorithmic question for
MDPs is then: what order should one evaluate rationality coefficients.
We propose a three staged sequence: (i) Compute $x_\rat$ for the end
points $\rat \in \{0, \infty\}$.  (ii) Double $\rat$ until $h_\rat \leq
\randomness$, yielding $\rat_1\ldots \rat_j$, where $\rat_1 = 1$.
(iii) Binary search for $\rat \in [\rat_{j-1}, \rat{j}]$.
\begin{mdframed}
  Our approximation scheme yields a semi-decision process which halts
  iff either (a) $\langle \scthreshold, \randomness \rangle$ is
  bounded away from $\pareto{\solutions}$ \emph{or} (b)
  $\langle \scthreshold, \randomness \rangle$ is visited by
  $x_{\rat_i}$.
\end{mdframed}
Next, observe that given a maximum resolution, $\kappa$, in
rationality, this approximation scheme becomes linear in the MDP size
and logarithmic in the final rationality coefficient $\rat_*$ and the
resolution $\kappa$, i.e., the run-time is,
\begin{equation}
  O\bigg(\hspace{-1.4em}\underbrace{|\sg|}_{\text{Evaluate Point $x_\rat$}}\hspace{-1.4em}\cdot\overbrace{\log(\rat_*)}^{\text{Doubling Phase}}\cdot\underbrace{\log(\nicefrac{1}{\kappa})}_{\text{Binary Search}}\bigg)
\end{equation}
Finally, before generalizing to stochastic games, we observe that in
practice, $\sched_{100} \approx \sched_\infty$, and one can often take
$\rat_* \leq 100$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
