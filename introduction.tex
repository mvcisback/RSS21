% Declarative Constraints ar neat idea.
The use of declarative specifications, e.g. in the form of temporal logic formulas, has become a popular way to construct high-level robot controllers~\cite{DBLP:conf/iros/HorowitzWM14, DBLP:conf/rss/WongEK14, DBLP:conf/iros/HeLKV17, DBLP:conf/icra/FuATP16, DBLP:conf/icra/HeWKV19, DBLP:journals/arobots/MoarrefK20, DBLP:conf/icra/KantarosM0P20}.
% Synthesis closes the gap.
Given a user provided specification, \emph{synthesis} algorithms aim
to automatically create a control policy that ensures that the
specification is met, or explain why such a policy does not
exist. Together, synthesis and declarative specifications facilitate
quickly and intuitively solving a wide variety of control tasks.  For
example, consider a delivery drone operating in a workspace. One may
specify the drone should ``within 10 minutes, visit four locations (in any
order) \emph{and} avoid crashing.''. A synthesis tool may then create a
finite state controller which guarantees this specification is met,
under a particular world model.
% Declarative Synthesis need not produce variety.  
Importantly, while many controllers may conform to the
provided specification, many synthesis algorithms provide a
single, often deterministic, policy.  For instance, in our drone
example, a synthesized controller may generate only a single path
through the workspace.

% On the importance of being varied.
In some settings, such policies however are undesirable.  First, in
many tasks, the predictability (or bias) of the policy may be a
liability.  Examples include
patrolling~\cite{DBLP:journals/ior/AlpernMP11}, behavior prediction
and inference~\cite{DBLP:conf/cav/Vazquez-Chanlatte20}, and creating
controller harnesses for fuzz testing (see motivating
example). Second, synthesis algorithms work on \emph{idealized}
models, and thus any policy that overcommits to any given model quirk
may in practice yield poor performance. In such settings,
randomization is known to make policies more robust against worst-case
deviations~\cite{mceThesis, maxEntAnswer}. Unfortunately, classical 
synthesis methods result in policies that need not (and typically do
not) exhibit randomization.

% Propose CI and highlight new features.
To address these potential deficits, we advocate for the adoption of
the recently proposed control
improvisation~\cite{DBLP:conf/cav/FremontS18,DBLP:conf/fsttcs/FremontDSW15}
framework, in which one specifies a controller with three types of
declarative constraints. (i) \emph{Hard constraints} that, as in the
classical setting, must be satisfied, (ii) \emph{soft constraints} that
on most executions should hold, and (iii) \emph{randomization
constraints} that ensure that a synthesized policy does not overcommit to a particular action or behavior. 
The key challenge when solving control improvisation is that randomization and performance, in the form of soft constraints, constitute a natural trade-off.

Unfortunately, control improvisation has so far been limited to
nondeterministic domains where uncertainty is resolved
adversarially. This assumption is often too restrictive and leads
(together with the soft/hard constraints) to conservative policies or
common situations in which the synthesis algorithm cannot be employed
at all. To overcome this weakness, we develop a theory of control
improvisation in stochastic games which admit
arbitrary \emph{combinations} of nondeterministic and probabilistic
uncertainty, including unknown or imprecise transition
probabilities. 

Technically, we formulate our problem on \emph{simple stochastic
games}~\cite{DBLP:conf/dimacs/Condon90}, an extension of \emph{Markov decision processes} (MDPs) that divides states
between controllable states and uncontrollable (or adversarially
controlled) states. \emph{Soft constraints} are finite horizon
temporal properties with a threshold on the worst-case probability of
that the property holding by the end of the episode. \emph{Hard
constraints} are soft constraints to be satisfied with probability 1. In
contrast to other work on control improvisation, we adopt causal entropy as a natural means to formalize \emph{randomness
constraints}.  Causal entropy is a prominent notion in directed
information theory~\cite{DirectedInfoTheoery} that strongly correlates with robustness in the
(inverse) reinforcement learning setting~\cite{mceThesis,
maxEntAnswer}. We refer to this variant of control improvisation as
\emph{Entropic Reactive Control Improvisation} (ERCI) and show that ERCI
conservatively extends reactive control improvisation~\cite{DBLP:conf/cav/FremontS18} to stochastic
games. More precisely, entropy can
be used in the non-stochastic setting and yields results analogous to
reactive control improvisation. ERCI also extends  classical policy synthesis in stochastic games, i.e. synthesis in absence of randomness constraints as, e.g., implemented in PRISM-games~\cite{DBLP:journals/sttt/KwiatkowskaPW18}.


%We argue that soft constraints can naturally be considered as an
%optimization objective which one can trade-off for more randomization.
%Indeed, our method strongly relies on the computation of a
%Pareto-front that explores the trade-off between randomization and
%optimizing the soft constraint using the notion of rationality. This
%means that rather than asking the user to fix rather arbitrary
%threshold values for both types of constraints, we may visualize the
%trade-off between these two entities.
%
\mypara{Contributions}
In summary, this paper contributes ERCI, an algorithmic way to trade
performance and randomization in stochastic games. As we motivate in
the example below, games that combine both adversarial and
probabilistic behavior in an environment allows for modeling
flexibility, facilitate applicability to new domains. To support this
extension, the paper proposes and shows the benefits of formulating
randomization constraints with causal entropy.  Finally, this work
contributes the necessary technical machinery and a prototype 
implementation. Combined, our theoretical and empirical analysis
suggest that the ERCI framework contributes a tractable and flexible
modeling formalism.

\mypara{Overview} This paper is structured as follows. We begin with a
motivating example (Sec.~\ref{sec:motivating}). Then we provide
preliminaries and formalize the ERCI problem statement
(Sec.~\ref{sec:problem}). Next, we cast ERCI
as a multi-objective optimization problem and study properties of the
solution set (Sec.~\ref{sec:convex}). With this technical machinery developed,
Sec.~\ref{sec:mdps} re-frames existing literature on maximum causal
entropy inference and control to derive an algorithm for MDPs.  Then in Sec.~\ref{sec:sgs}, we provide an
algorithm for the general case of stochastic games. We conclude with
an empirical evaluation (Sec.~\ref{sec:empirical}) and a comparison
with related work, e.g., other control improvisation
formulations (Sec.~\ref{sec:related}). Proofs are attached in Sec.~\ref{sec:proofs}.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
