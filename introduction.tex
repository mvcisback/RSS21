% Declarative Constraints ar neat idea.
The use of declarative specifications to define high-level plans, e.g.,
automata and temporal logic formula, has become popular in recent
years~\cite{DBLP:conf/iros/HorowitzWM14, DBLP:conf/rss/WongEK14, DBLP:conf/iros/HeLKV17, DBLP:conf/icra/FuATP16, DBLP:conf/icra/HeWKV19, DBLP:journals/arobots/MoarrefK20, DBLP:conf/icra/KantarosM0P20}.
% Synthesis closes the gap.
Given a user provided specification, \emph{synthesis} algorithms aim
to automatically create a control policy that ensures that the
specification is met, or deliver feedback why such a policy does not
exist. Together, synthesis and declarative specifications facilitate
quickly and intuitively solving a wide various of control tasks.  For
example, consider a patrol drone operating in a workspace. One may
specify the drone should ``(within 10 minutes) visit locations (in any
order) AND avoid crashing.''. A synthesis tool may then create a
finite state controller which guarantees this specification is met,
under a particular world model.  
% Declarative Synthesis need not produce variety.  
Importantly, while there may be many controllers
that conform to the provided specification, many synthesis algorithms
will provide a single, often deterministic, policy.  For instance, in
our drone example, a synthesized controller may generate only a
single path through the workspace.

% On the importance of being varied.
Such policies have two weaknesses. First, in many tasks, the
predictability of the policy may be a liability, .e.g., a patrol guard
should be sufficiently random to prevent an adversarial observer to
easily guessing the remainder of their path. Second, synthesis
algorithms work on idealized models, and indeed while randomization is
a well-known ingredient to make policies or algorithm in general more
robust against worst-case
deviations~\cite{mceThesis, maxEntAnswer},
these synthesized policies need not exhibit randomization.

% Propose CI and highlight new features.
These observations lead us to advocate for the adoption of control
improvisation in which one assumes three types of declarative
constraints. \emph{Hard constraints} that, as in the classical
setting, must be satisfied, \emph{soft constraints} that on most
executions should hold, and \emph{randomization constraints} that
ensure that a synthesized policy does not overly commit to a
particular action or behavior. The idea of control improvisation is
not new~\cite{DBLP:conf/cav/FremontS18,DBLP:conf/fsttcs/FremontDSW15},
but has been limited to deterministic domains where uncertainty is
resolved adversarialy. These assumption are too restrictive and lead
(together with the soft/hard constraints) to conservative policies or
situations in which the synthesis algorithm cannot be employed at
all. To overcome this weakness, we develop a theory of control
improvisation in stochastic games which admit combinations of
adversarial and probabilistic uncertainty, e.g., unknown or imprecise
transition probabilities. As we show later, this extension requires a
different view on randomization constraints, as the policy is no
longer the only source of randomization.

Technically, we formulate our problem on \emph{simple stochastic
games}, an extension of Markov decision processes that divides states
between controllable states and uncontrollable (or adverserially
controlled) states. \emph{Soft constraints} are finite horizon
temporal properties with a threshold on the worst-case probability of
that the property holding by the end of the episode. \emph{Hard
constraints} are soft constraints satisfied with probability 1. In
contrast to other work on Control Improvisation, we adopt the notion
of causal entropy as natural means to formalize \emph{randomness
constraints}.  Causal entropy is a prominent notion in directed
information theory that strongly correlates with robustness in the
(inverse) reinforcement learning setting~\cite{mceThesis,
maxEntAnswer}. We refer to this variant of control improvisation as
Entropic Reactive Control Improvisation (ERCI) and show that ERCI
conservatively extends reactive control improvisation to stochastic
games. More precisely, while we focus on stochastic games, entropy can
be used in the non-stochastic setting and yields results analogous to
the reactive control improvisation.


%We argue that soft constraints can naturally be considered as an
%optimization objective which one can trade-off for more randomization.
%Indeed, our method strongly relies on the computation of a
%Pareto-front that explores the trade-off between randomization and
%optimizing the soft constraint using the notion of rationality. This
%means that rather than asking the user to fix rather arbitrary
%threshold values for both types of constraints, we may visualize the
%trade-off between these two entities.
%
In summary, this paper contributes ERCI, an algorithmic way to trade
performance and randomization in stochastic games. The support for
stochastic games that combine both adversarial and probabilistic
behavior in an environment allows for modeling flexibility,
admitting applicability to new domains. To support this extension,
the paper shows that one benefits from a completely different notion
of randomization constraints based on causal entropy.  This paper
contributes the necessary machinery as well as a prototypical
implementation. Finally, the paper shows its applicability based on a
motivating example discussed in Sec.~\ref{sec:motivating}, before
formalizing the problem statement in Sec.~\ref{sec:problem} and then
discussing the algorithm, first on the special case of MDPs in
Sec.~\ref{sec:mdps}, discussing its properties in Sec.~\ref{sec:convex}, and then for the general case of stochastic games
in Sec.~\ref{sec:sgs}.  We show an empirical evaluation in
Sec~\ref{sec:empirical} and discuss related work at the end of the
paper, in Sec.~\ref{sec:related}.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
