The use of declarative constraints, often in the form of linear temporal logic constraints, has become popular in recent years. 
Strong \emph{synthesis} algorithms support the automatic creation of a control policy that ensures that the declarative constraints are met, or deliver feedback why such a policy does not exist. 
The use of such algorithms takes away the burden of a user implementing these controllers, and allows one to quickly and intuitively define a variety of behaviors.
We notice that typically, the declarative constraints allow for a variety of controllers that all equally satisfy the constraints, yet synthesis algorithms create a single, often deterministic policy. 
Such policies have two weaknesses. First, in scenarios where predictability is an issue, the policy of a patrolling guard should be sufficiently random to prevent an adverserial observer to easily observe the used policy. 
Second, the synthesis algorithms work on idealized models, and indeed  randomization is a well-known ingredient to make policies or algorithm in general more robust against worst-case deviations. 


The idea of control improvisation is not new, but so far can only assume an adverserial environment. This assumption is too pessimistic, as not all agents 

We formulate our problem on \emph{simple stochastic games}, an extension of MDPs that divides states between controllable states and uncontrollable states. 
In particular, this means that we can 



\paragraph{Contribution}

\begin{itemize}

\item We conservatively genalize control improvisation from the deterministic to the stochastic setting (MDPs) by adopting ideas from specification inference.
\item We further conservatively extend these ideas to support reactive control improvisation in stochastic games
\item We contrast control improvisation for stochastic games with policy sampling, i.e., with uniformly sampling schedulers satisfying given temporal constraints.
\item We provide a scalable prototypical implementation.
\end{itemize}

