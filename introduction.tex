% Declarative Constraints ar neat idea.
The use of declarative specifications to define high-level plans, e.g.,
automata and temporal logic formula, has become popular in recent
years~\cite{DBLP:conf/iros/HorowitzWM14, DBLP:conf/rss/WongEK14, DBLP:conf/iros/HeLKV17, DBLP:conf/icra/FuATP16, DBLP:conf/icra/HeWKV19, DBLP:journals/arobots/MoarrefK20, DBLP:conf/icra/KantarosM0P20}.
% Synthesis closes the gap.
Given a user provided specification, \emph{synthesis} algorithms aim
to automatically create a control policy that ensures that the
specification is met, or deliver feedback why such a policy does not
exist. Together, synthesis and declarative specifications facilitate
quickly and intuitively solving a wide various of control tasks.  For
example, consider a patrol drone operating in a workspace. One may
specify the drone should ``(within 10 minutes) visit locations (in any
order) AND avoid crashing.''. A synthesis tool may then create a
finite state controller which guarantees this specification is met,
under a particular world model.  % Declarative Synthesis need not
produce variety.  Importantly, while there may be many controllers
that conform to the provided specification, many synthesis algorithms
will provide a single, often deterministic, policy.  For instance, in
our drone example, a synthesized controller may generate only a
single path through the workspace.

% On the importance of being varied.
Such policies have two weaknesses. First, in many tasks, the
predictability of the policy may be a liability, .e.g., a patrol guard
should be sufficiently random to prevent an adversarial observer to
easily guessing the remainder of their path. Second, synthesis
algorithms work on idealized models, and indeed while randomization is
a well-known ingredient to make policies or algorithm in general more
robust against worst-case
deviations~\cite{mceThesis, maxEntAnswer},
these synthesized policies need not exhibit randomization.

These observations lead us to advocate for the adoption of control
improvisation in which one assumes three types of declarative
constraints. \emph{Hard constraints} that, as in the classical
setting, must be satisfied, \emph{soft constraints} that on most
executions should hold, and \emph{randomization constraints} that
ensure that a synthesized policy does not overly commit to a
particular action or behavior. The idea of control improvisation is
not new~\cite{DBLP:conf/cav/FremontS18,DBLP:conf/fsttcs/FremontDSW15},
but has been limited to deterministic domains where uncertainty is
resolved adversarialy. These assumption are too restrictive and lead
(together with the soft/hard constraints) to conservative policies or
situations in which the synthesis algorithm cannot be employed at
all. To overcome this weakness, we develop a theory of control
improvisation in stochastic games which admit combinations of
adversarial and probabilistic uncertainty, e.g., unknown or imprecise
transition probabilities. As we show later, this extension requires a
different view on randomization constraints, as the policy is no
longer the only source of randomization.

Technically, we formulate our problem on \emph{simple stochastic
games}, an extension of Markov decision processes that divides states
between controllable states and uncontrollable (or adverserially
controlled) states. Hard constraints are finite horizon temporal
properties that restrict the policy such that together with any
adverserial policy, the property is satisfied with probability
one. Soft constraints are finite horizon temporal properties with a
user defined threshold on the probability that the property holds on
the path the complete system exposes.  We adopt the notion of causal
entropy as natural means to ensure randomization. Causal entropy is a
prominent notion in information theory that strongly correlates with
robustness in the inverse reinforcement learning setting. We show that
it conservatively extends the reactive control improvisation to
stochastic games. More precisely, while we focus on stochastic games,
entropy can be used in the non-stochastic setting and yields results
analogous to the reactive control improvisation.

We argue that a soft constraints can naturally be considered as an
optimization objective which one can trade-off for more randomization.
Indeed, our method strongly relies on the computation of a
Pareto-front that explores the trade-off between randomization and
optimizing the soft constraint using the notion of rationality. This
means that rather than asking the user to fix rather arbitrary
threshold values for both types of constraints, we may visualize the
trade-off between these two entities.

Together, this paper contributes entropy-guided control improvisation, an algorithmic way to trade performance and randomization in stochastic games. 
The support for stochastic games that combine both adversarial and probabilistic behavior in an environment allows for crucial modeling flexibility and enables the applicability to new domains. 
Entropy-guided control improvisation is based on using causal entropy as basis for the randomization constraints.
To support this extension, the paper shows that one must adapt a completely different notion of randomization constraints. This paper contributes the necessary machinery as well as a prototypical implementation.
Finally, the paper shows its applicability based on a motivating example discussed in Sec.~\ref{sec:motivating}, before formalizing the problem statement in Sec.~\ref{sec:problem} and then discussing the algorithm, first on the special case of MDPs in Sec.~\ref{sec:mdps} and then for the general case of stochastic games in Sec.~\ref{sec:sgs}. 
We show an empirical evaluation in Sec~\ref{sec:empirical} and discuss related work at the end of the paper, in Sec.~\ref{sec:related}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
