
\begin{figure}
  \centering \scalebox{0.33}{
    \import{imgs/}{motivating_example.pdf_tex} }
  \caption{ Illustration of delivery drone testing example. The goal
    is to synthesize a policy for the bottom left (white circle) drone
    to test the controller of the top right (black square) drone. Ideally,
    the synthesized policy should be as randomized as possible to avoid
    testing bias.\label{fig:motivating} }
\end{figure}

% Setup Testing Premise.
We consider a scenario in which a regulatory agency
wishes to certify the safety and performance of a new delivery drone $\droneEnv$.
As part of the process, the agency runs $\droneEnv$ through a series of tests. For example, given a certain
delivery route, the agency investigates whether $\droneEnv$ successfully delivers packages while
avoiding \emph{other} delivery drones. To execute this test, the agency decides to
synthesize a controller for another delivery drone, $\droneEgo$, to test if
$\droneEnv$ can be certified.

% Paint a picture.
Concretely,
% suppose we have high-level models (e.g., motion planners)
%for $\droneEnv$ and $\droneEgo$, 
suppose we command
$\droneEnv$ to continuously visit four houses in our workspace. We
illustrate such a scenario in~Fig.~\ref{fig:motivating}, in which
$\droneEnv$ and $\droneEgo$ are shown as black square and white circle
drones respectively.  For this test scenario, the regulatory agency,
wishes to exam how $\droneEnv$ responds to delivering packages to the
red houses in the presence of $\droneEgo$. Namely, the
agency wishes to have $\droneEgo$ also deliver packages while avoiding
$\droneEnv$. Importantly,
to properly exercise $\droneEnv$, $\droneEgo$ should show a \emph{variety} of behaviors meeting  specification, and the behaviors should not be biased to any behavior beyond the given specification.
% Ideally, this $\droneEgo$'s policy should be as
%un-biased as possible to exercise $\droneEnv$ on a variety of
%scenarios, and ideally capture and sensor, software, or hardware
%errors, e.g., a bug in a machine learned perception sensor.

% Frame as RCI.
With the ERCI framework, the agency may formalize the above scenario with the
following constraints on $\droneEgo$:
\begin{enumerate}
\item (\emph{hard constraint}) Ensure that the two drones \emph{never} collide.
\item (\emph{soft constraint}) With probability at least $.8$, visit all four houses within 10 minutes.
\item (\emph{randomness constraint}) Perform this task as unpredictability as possible.
\end{enumerate}
What  remains is to synthesize a controller given the constraints
\emph{and} the world model. At this point, it is worth examining more
closely how one models $\droneEnv$'s controller when synthesizing
$\droneEgo$. We illustrate by examining three models. In all models, we capture the behaviors of $\droneEnv$ and $\droneEgo$. We focus on $\droneEnv$, but the ideas carry over to modeling the actuation of $\droneEgo$.
%
%\mypara{Deterministic Model}
%In the simplest case, the manufacture might guarantee that a
%$\droneEgo$ will deliver packages on a fixed route.\sj{But it also needs to respond to other drones, so I do not find this fixed route particularly convincing. } While tempting to
%believe, such a route may be hard to a-priori compute and sensitive to
%the exact details of the test workspace.

\mypara{Nondeterministic Model}
The simplest approach to modelling is not to make any assumptions about  $\droneEnv$ beyond what already has been established. We consider here that the houses are visited either clock-wise or counter-wise order but that it may switch direction at \emph{any time}. 
Such a model is too liberal and our assumptions will be too pessimistic, which leads to a bad test set.
First, if $\droneEnv$ is unrestricted, then $\droneEgo$'s behavior is severely limited. This limitation restricts the variance of its behavior, and it will not test $\droneEnv$'s true behavior. 
A purely non-deterministic model for $\droneEnv$ thus may not lead to the synthesis of adequate behavior for $\droneEgo$. 
 %Such a model is clearly too pessimistic -- there is no policy that satisfies
%our constraints -- and frankly unrealistic within the context.
%Furthermore, even if there existed a winning policy, such a pessimistic
%model would result in an unnecessarily conservative (and thus predictable)
%test controller, limiting its utility.

\mypara{Stochastic Model}
Rather than the pessimistic nondeterministic (or adversarial)  assumption, we may collect data about $\droneEnv$ and construct a stochastic model, e.g., using inverse reinforcement learning~\cite{DBLP:conf/icml/NgR00}.
Concretely (but simplified), after examining the data, one observes that $\droneEnv$ appears
to flip a biased coin whenever it reaches a house to decide whether or
not to turn around. 
This models $\droneEnv$ much more precisely, and allows for more targeted test by $\droneEgo$.

%The resulting Markov Decision Process may be
%sufficiently correct to synthesize adequate improvisers without
%being too pessimistic.

\mypara{Nondeterministic and Stochastic Model} However, a natural criticism for stochastic
models is the dependence on \emph{fixed} probabilities. 
Obtaining such probabilities with confidence requires many tests which defeat the purpose of our test setup, and making point-estimates from little data may not create faithful models of the actual behavior.
% In our
%example, we may have observed $\droneEnv$'s behavior and extracted
%(point-)estimate probabilities, but these probabilities may still have
%non-trivial error margins or could be sensitive to changes in the
%workspace.
  In absence of enough (or reliable) data, we can arbitrarily combine
nondeterministic choices and stochastic behavior. We may use stochastic abstractions for parts that we can faithfully model, and nondeterministic behavior in absence of data.
In particular, we
support interval-valued transition probabilities.  Consider the
delivery-drone $\droneEnv$. Rather than inferring a point-estimate
from data, we may have inferred that the probability of turning around
is in the interval $[p - \varepsilon, p + \varepsilon]$ for adequate
values of $p$ and $\varepsilon$.  Furthermore the actual probability
may even depend on aspects of the current state.

% Bring it all together.
\mypara{ERCI as a unifying framework}
The strength of the (entropy-guided) control improvisation framework
is that we can combine all these aspects into a single (flexible)
computational model. 
In particular, the models above are captured by a 2-player game, a 1.5-player game (MDP) and a 2.5-player game (stochastic game), respectively.
In all cases, the first player controls the behavior of $\droneEgo$ and this controller is to be synthesized. 
We contribute an algorithm that synthesizes that controller such that
it  maximally randomizes for all of the models discussed above. In the
coming sections, we shall formally define the ERCI problem, highlight
that there is an implicit trade-off between performance of the soft
constraint and unpredictability, and provide algorithms to solve ERCI
in Stochastic Games.

% Pareto-curve that shows how randomization and performance yield a tradeoff. This means that rather than a-priori selecting threshold for entropy and the probability of not running out of battery, we obtain a variety op options and select the trade-off that is most satisfactory. Consider Fig.~\ref{}.


% We consider high-level planning for drones. We consider a setting with a controllable drone $D$ and in presence of a secondary drone $E$. 
% We partition the airspace into different zones. Four zones are marked as special points of interest (POIs).
% One of the zones in a corner is a recharge station, where $D$ initially starts. $E$ starts in the opposite corner. We assume perfect observability.  
% For safety, our plan must (\emph{hard constraint}) ensure that the two drones are never in the same zone. We are only interested in plans that visit the four POIs within a given time horizon (\emph{hard constraint}).
% We should (\emph{soft constraint}) ensure that we do not run out of battery with high probability. 
% Our aim is to create a plan such that the paths of $D$ within its environment are maximally unpredictable (random constraint), i.e., we want to maximally randomize over the paths that satisfy the constraints. Due to the nature of the soft constraint, some of the paths that we include may violate this constraint.  We apply our novel entropy-guided control improvisation. In the folllowing, we discuss different aspects of this setting.

% Let us first start in absence of $E$. 
% The main task here is to ensure power-aware scheduling, i.e., depending on the state of charge of the battery, we want to adapt our plan. Crucial in this is an adequate model of the battery. 
% Both the battery quality itself and the power consumption are, however, uncertain. 
% In particular, we may model that in every step deterministically the average power is consumed, but this plan will not be robust to any other behavior, and the plan is unrealistic~\cite{DBLP:conf/cyphy/HermannsKN15}. Typically, in synthesis, the other extreme is assumed (any amount of power can be drawn in every step). In this setting, this assumption leads the battery to be discharged with the maximal power consumption -- while this is clearly too pessimistic -- there is no policy that satisfies our constraints.
% We use a model in which we discretize the battery charge, and in every time step the battery charge decrements by one step with some probability $p$. 
% Overall, this yields a binomial distribution over the maximal steps until the battery is depleted.
% In Fig.~\ref{fig:motivating:batteries}, we show paths with a larger and smaller battery. As to be expected, the larger battery allows for more freedom in randomizing, as it is easier to meet the soft constraint.

% Orthogonally, let us consider drone $E$. 
% In the best case, drone $E$ is a delivery drone delivering packages along a fixed route. We can encode this route into the model. 
% Compare Fig.~\ref{} without a drone $E$ and Fig.~\ref{} with drone $E$ flying the path marked in red. 
% We can see how fewer paths meet the hard constraint, and thus, $D$ randomizes over fewer paths. 
% The plans in Fig.~\ref{} are not very robust: what if $E$ occasionally decided to return to its base (e.g., to recharge). More precisely, in Fig.~\ref{} we illustrate the policy for $D$ if we assume that $E$ flips a biased coin in every step in which it decided to turn around.
% We observe that this decreases the paths that satisfy the hard guarantee (not crashing) and indirectly also means that it becomes more likely that we deplete the battery due to evading $E$.
% Finally, rather than assuming some stochastic behavior where $E$ turns around, we may want to not make any assumption on under which circumstances or what probability $E$ turns. 
% The difference is more subtle: changing from randomization to adversarial behavior of $E$ does not change the set of paths that violate the hard constraint, $D$ will need to evade $E$ more often, yielding higher battery consumption. 
% More generally, the difference between assuming random behavior of $E$ and adversarial behavior is as follows: In the latter case, we are interested in a policy that is good for any behavior of $E$, that is, it is good in the worst-case, whereas assuming (uniform) random behavior for $E$ in expectation, but does not give guarantees on the worst-case. Generally, optimizing for the worst-case is overly pessimistic.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
