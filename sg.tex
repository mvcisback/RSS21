MDP algorithm in hand, we are now ready to provide an algorithm for
stochastic games. At a high level, this algorithm works by initially
planning for $\pTwo$ selecting the action that minimizes randomness
(with ties broken by performance). This assumption leads to a
reduction to the MDP-case, where the rationality indexed family,
$\sched_\rat$, indexes the Pareto Front.  If this assumption is ever
violated, the resulting state must support more randomness.  The
rationality, $\rat$, is thus lowered to match the worst case randomness,
which due to monotonicity of $\solfuncp$, can only increase
performance. Surprisingly, as we shall later prove, this class of
policies indexes the Pareto Front for SGs!

\mypara{Deterministic Adversaries}
First and foremost, observe that to render an ERCI instance
un-achievable, it suffices for $\pTwo$ to violate
either the performance threshold OR the randomness threshold.  Next,
notice that because $\pOneSched$ is a priori fixed, $\pTwoSched$ can
be seen as repeatedly selecting between a convex combination of
$\rndp$ (and $\scp$) for the corresponding sub-graph. As the maximum of a
convex combination is always achievable on the boundaries, we can
w.l.o.g. assume that $\pTwoSched$ is \emph{deterministic}.  Similarly,
notice that given a fixed $\pOneSched$, the worst-case $\pTwoSched$
response can be computed via dynamic programming in topological order
from the leafs to the root of $\sg$.

\mypara{MDP subroutine}
On the other hand, suppose $\pTwoSched$ was known, reducing the SG to
a MDP. As discussed in the previous section, for the MDP case, it
suffices to consider rationality indexed policies. Let use denote the
resulting MDP and maximum causal entropy family of (partial) policies
as $\sg[\pTwoSched]$ and $\pi^\pOne_\rat[\pTwoSched]$ resp. Now
suppose $\rat$ is a priori fixed. Again, via a topological ordered
evaluation of states from the leaves to the root, one can compute the
$\pTwo$ policy, call $\sched_\rat^\pTwo$, that minimizes the maximum
causal entropy policy family, $\pi^\pOne_\rat[~.~]$. By
We shall refer to resulting (partial) schedule as: $\pi_\rat \eqdef \langle
\pi^\pOne_\rat[\sched_\rat^\pTwo] , \sched_\rat^\pTwo \rangle$.

\mypara{Recursively Matching Randomness}
Of course, even if $\rat$ is fixed, $\pTwoSched$ need not be
$\sched^\pTwo_\rat$. Nevertheless, observe that by selecting the worst
entropy MDP, $\pi_\rat$ establishes an achievable randomness for the
sub-game rooted at each state, call $\delta_\rat(s)$. Now suppose,
$\pTwo$ deviates from $\sched_\rat^\pTwo$ at state $s$. Note that, so
long as $\pOneSched$ does not result yield randomness less than
$\delta_\rat(s)$, the worst-case randomness will not decrease.
This begs the question ``what maximum performance can one guarantee at $s' \in
\Succ(s)$ given randomness $\delta_\rat(s)$?''
\begin{mdframed}
  The key observation is that each state $s'$ is the root of a sub
  game, $\sg[s]$, with a corresponding Pareto Front,
  $\pareto{\solutions}[s]$.
\end{mdframed}
In particular, given access the characteristic functions,
$f_\solutions^{s'}$, for each $s' \in \Succ(s)$, one can compute:
\begin{equation}\label{eq:performance_lookup}
  \epsilon_\rat(s) = \min_{s'} f_\solutions^{s'}(\delta_\rat(s)).
\end{equation}
Thus, via dynamic programming, one can define $\epsilon_\rat(\iota)$.
Moreover, assuming that this entropy matching family of policies
indexes $\pareto{\solutions}$ (proved in Lemma ?), one can handle
deviations from $\sigma^\pTwo_\rat$ by ``replanning''. Namely, one
extends $\pi_\rat$ at $s'$ by computing a new rationality coefficient,
$\rat'$, such that:
\begin{equation}
  \delta_{\rat'}(s') = \delta_{\rat}(s)
\end{equation}
Due to the monotonicity $\rat \leq \rat'$, and thus $\epsilon_\rat \leq \epsilon_{\rat'}$, 
Therefore, ignoring the feasibility of computing the exact Pareto
Front, we obtain a synthesis algorithm for SGs!

\mypara{Approximate Pareto Fronts}
Of course, by varying $\rat$, one can only construct approximate
Pareto Fronts. We now
adapt the above algorithm to the case where on has access to
$\kappa$-close Pareto Front under-approximations,
$\hat{f}_\solutions^{s'}$, where $\kappa$ bounds the $\infty$-norm
error. In particular, observe that if $\epsilon_\rat(s)$ is known
within $\kappa$, convex combinations of these intervals cannot
increase the error beyond $\kappa$, i.e.,
\begin{equation}
  q\cdot[x, x + \kappa] + \bar{q}\cdot[y, y + \kappa] = [z, z + \kappa],
\end{equation}
where $z = q\cdot x + \bar{q}\cdot y$. Thus, since $\pi_\rat$ and
$P(s, a)$ are independent of this Pareto Front (and manipulate
performance via convex combinations) the error can only accumulate on
the Pareto Fronts for $s \in S_\pTwo$. Notice then that so long as,
$\kappa\cdot\tau$ is enough resolution to answer $\scp_\rat <
\scthreshold$, one obtains a semi-decision procedure as in the MDP
case.

We then propose the following high-level algorithm:
\begin{mdframed}
\begin{enumerate}
\item Let $0 < \kappa < 1$ be some arbitrary initial tolerance.
\item Recursively compute $\kappa$-close Pareto Fronts for each state
in $s'$ using causal entropy matching.
\item If $\scthreshold$ is within $\kappa\cdot \tau$ distance to $\hat{\pareto_{\solutions}}$,
  halve $\kappa$ and repeat.
\end{enumerate}  
\end{mdframed}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
