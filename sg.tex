MDP algorithm in hand, we are now ready to provide an algorithm for
stochastic games. At a high level, this algorithm works by initially
planning for $\pTwo$ selecting the action that minimizes randomness
(with ties broken by performance). This assumption leads to a
reduction to the MDP-case, where the rationality indexed family,
$\sched_\rat$, indexes the Pareto Front.  If this assumption is ever
violated, the resulting state must support more randomness.  The
rationality, $\rat$, is thus lowered to match the worst case randomness,
which due to monotonicity of $\solfuncp$, can only increase
performance. Surprisingly, as we shall later prove, this class of
policies indexes the Pareto Front for SGs!

\mypara{Deterministic Adversaries}
First and foremost, observe that to render an ERCI instance
un-achievable, it suffices for $\pTwo$ to violate
either the performance threshold OR the randomness threshold.  Next,
notice that because $\pOneSched$ is a priori fixed, $\pTwoSched$ can
be seen as repeatedly selecting between a convex combination of
$\rndp$ (and $\scp$) for the corresponding sub-graph. As the maximum of a
convex combination is always achievable on the boundaries, we can
w.l.o.g. assume that $\pTwoSched$ is \emph{deterministic}.  Similarly,
notice that given a fixed $\pOneSched$, the worst-case $\pTwoSched$
response can be computed via dynamic programming in topological order
from the leafs to the root of $\sg$.

\mypara{MDP subroutine}
On the other hand, suppose $\pTwoSched$ was known, reducing the SG to
a MDP. As discussed in the previous section, for the MDP case, it
suffices to consider rationality indexed policies. Let use denote the
resulting MDP and maximum causal entropy family of (partial) policies
as $\sg[\pTwoSched]$ and $\pi^\pOne_\rat[\pTwoSched]$ resp. Now
suppose $\rat$ is a priori fixed. Again, via a topological ordered
evaluation of states from the leaves to the root, one can compute the
$\pTwo$ policy, call $\sched_\rat^\pTwo$, that minimizes the maximum
causal entropy policy family, $\pi^\pOne_\rat[~.~]$. By
We shall refer to resulting (partial) schedule as: $\pi_\rat \eqdef \langle
\pi^\pOne_\rat[\sched_\rat^\pTwo] , \sched_\rat^\pTwo \rangle$.

\mypara{Recursively Matching Randomness}
Of course, even if $\rat$ is fixed, $\pTwoSched$ need not be
$\sched^\pTwo_\rat$. Nevertheless, observe that by selecting the worst
entropy MDP, $\pi_\rat$ establishes an achievable randomness for the
sub-game rooted at each state, call $\delta_\rat(s)$. Now suppose,
$\pTwo$ deviates from $\sched_\rat^\pTwo$ at state $s$. Note that, so
long as $\pOneSched$ does not result yield randomness less than
$\delta_\rat(s)$, the worst-case randomness will not decrease.
This begs the question ``what maximum performance can one guarantee at $s' \in
\Succ(s)$ given randomness $\delta_\rat(s)$?''
\begin{mdframed}
  The key observation is that each state $s'$ is the root of a sub
  game, $\sg[s]$, with a corresponding Pareto Front,
  $\pareto{\solutions}[s]$.
\end{mdframed}
In particular, given access the characteristic functions,
$f_\solutions^{s'}$, for each $s' \in \Succ(s)$, one can compute:
\begin{equation}\label{eq:performance_lookup}
  \epsilon_\rat(s) = \min_{s'} f_\solutions^{s'}(\delta_\rat(s)).
\end{equation}
Thus, via dynamic programming, one can define $\epsilon_\rat(\iota)$.
Moreover, assuming that this entropy matching family of policies
indexes $\pareto{\solutions}$ (proved in Lemma ?), one can handle
deviations from $\sigma^\pTwo_\rat$ by ``replanning''. Namely, one
extends $\pi_\rat$ at $s'$ by computing a new rationality coefficient,
$\rat'$, such that:
\begin{equation}
  \delta_{\rat'}(s') = \delta_{\rat}(s)
\end{equation}
Due to the monotonicity $\rat \leq \rat'$, and thus $\epsilon_\rat \leq \epsilon_{\rat'}$, 
Therefore, ignoring the feasibility of computing the exact Pareto
Front, we obtain a synthesis algorithm for SGs!

\mypara{Approximate Pareto Fronts}
Of course, by varying $\rat$, one can only construct approximate
Pareto Fronts. We now
adapt the above algorithm to the case where one has access to
$\kappa$-close Pareto Front under-approximations,
$\hat{f}_\solutions^{s'}$, where $\kappa$ bounds the $\infty$-norm
error. In particular, observe that if $\epsilon_\rat(s)$ is known
within $\kappa$, convex combinations of these intervals cannot
increase the error beyond $\kappa$, i.e.,
\begin{equation}
  q\cdot[x, x + \kappa] + \bar{q}\cdot[y, y + \kappa] = [z, z + \kappa],
\end{equation}
where $z = q\cdot x + \bar{q}\cdot y$. Thus, since $\pi_\rat$ and
$P(s, a)$ are independent of this Pareto Front (and manipulate
performance via convex combinations) the error can only accumulate on
the Pareto Fronts for $s \in S_\pTwo$. Notice then that so long as,
$\kappa\cdot\tau$ is enough resolution to answer $\scp_\rat <
\scthreshold$, one obtains a semi-decision procedure as in the MDP
case.

\begin{mdframed}
  We then propose the following high-level algorithm:
\begin{enumerate}
\item Let $0 < \kappa < 1$ be some arbitrary initial tolerance.
\item Recursively compute $\kappa$-close Pareto Fronts for each state
in $s'$ using causal entropy matching.
\item If $\scthreshold$ is within $\kappa\cdot \tau$ distance to $\hat{\pareto{\solutions}}$,
  halve $\kappa$ and repeat.
\end{enumerate}  
\end{mdframed}
\mypara{Run-time and Decidability}
In practice, this algorithm can be significantly improved by adaptive
tolerances, lazily computing the Pareto Fronts, and only computing
Pareto Fronts for non-deterministic $\pTwo$ states. Nevertheless,
already this na\"ive algorithm gives a sense of the run-time
bottlenecks. Namely, if $\kappa^*$ tolerance is required to terminate,
then the $\kappa$ search introduces $O(\log(\nicefrac{1}{\kappa^*}))$
overhead. Furthermore, by computing $O(|\sg|)$ Pareto fronts, from the
leaves, one ensures that the complexity grows linearly with the graph
size - although the multiplicative constant depends on the number of
rationality coefficients explored per Pareto-front. We found that most
of the rationality coefficients explored were shared, which in
practice seems to amortize the cost per state. Finally, as in the
MDP-case, the resulting algorithm is only a semi-decision procedure,
being decidable if $\langle \scthreshold, \randomness \rangle$ is
bounded away from the root Pareto Front. As the Pareto Front has
measure 0, we argue that this is merely a technical concern, as a
small perturbation to the ERCI instance (i.e. a Smoothed
Analysis~\cite{SmoothedAnalysis}) or $\sg$ admits decidability.

\mypara{Soundness and Completeness} Our algorithm restricts itself to
considering only recursive entropy matching policies,
$\{\sched^{\pOne}_\rat\}_\rat$. Importantly, observe that because
fixing a policy yields a verifiable point in $\solutions$,
we can restrict ourselves to the case in which our algorithm claims
the ERCI instance un-achievable.
Below, we shall assume that our
algorithm claims the ERCI is un-achievable, but there exists
another policy not in $\{\sched^{\pOne}_\rat\}_\rat$, that achieves
$\langle \scthreshold, \randomness \rangle$.

\begin{proof}[Proof Sketch]
  First, observe that on games with only sink nodes, completeness
  follows directly. Now suppose the entropy matching family is
  complete on all sub-graphs of $\sg$ and for the sake of
  contradiction that, assume that,
  \begin{align}
    &\forall \sched_\pOne \in \{\sched^{\pOne}_\rat\}_\rat~.~ x_{\sched_{\pOne}} \prec \langle \scthreshold, \randomness \rangle\label{eq:reject}\\
    &\exists \sched_\pOne^* \notin \{\sched^{\pOne}_\rat\}_\rat~.~  \langle \scthreshold, \randomness \rangle \prec x_{\sched_{\pOne}^*}\label{eq:incomplete}.
  \end{align}
  Let $\sched_\pTwo^*$ denote the min-randomness $\pTwo$-policy given
  $\sched_\pOne^*$. On the MDP, $\sg[\sched_\pTwo^*]$, we know that
  that the soft-bellman backup family is complete, and thus there must
  exist some $\rat$ such that the partial policy $\pi_\rat$ matches
  the worst-case entropy of $\sched_\pOne^*$, i.e.,
  $\rndp_\rat = \rndp_{\sched_\pOne^*}$. Furthermore, because
  $\pi_\rat$ \emph{uniquely} maximizes causal entropy for its
  performance on $\sg[\sched_\pTwo^*]$, $\sched_\pOne^*$ must exactly
  match $\pi_\rat$ on $\sg[\sched_\pTwo^*]$. Thus, these policies must
  differ on some non-minimum entropy sub-graph. By the inductive
  hypothesis, we know that the entropy matching family is complete on
  these subgraphs, and thus if $\sched_\pOne^*$ achieves a given worst
  case performance on this sub graph, there must be an entropy
  matching that does so as well, contradicting
  assumptions~\eqref{eq:reject} and \eqref{eq:incomplete}.  Thus,
  entropy matching must be complete.
\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
