MDP algorithm in hand, we are now ready to provide an algorithm for
stochastic games. At a high level, this algorithm works by initially
planning for $\pTwo$ selecting the action that minimizes randomness
(with ties broken by performance). This assumption leads to a
reduction to the MDP-case, where the rationality indexed family,
$\sched_\rat$, indexes the Pareto Front.  If this assumption is ever
violated, the resulting state must support more randomness.  The
rationality, $\rat$, is thus lowered to match the worst case randomness,
which due to monotonicity of $\solfuncp$, can only increase
performance. Surprisingly, as we shall later prove, this class of
policies indexes the Pareto Front for SGs!

\mypara{Deterministic Adversaries}
First and foremost, observe that to render an ERCI instance
un-achievable, it suffices for $\pTwo$ to violate
either the performance threshold OR the randomness threshold.  Next,
notice that because $\pOneSched$ is a priori fixed, $\pTwoSched$ can
be seen as repeatedly selecting between a convex combination of
$\rndp$ (and $\scp$) for the corresponding sub-graph. As the maximum of a
convex combination is always achievable on the boundaries, we can
w.l.o.g. assume that $\pTwoSched$ is \emph{deterministic}.  Similarly,
notice that given a fixed $\pOneSched$, the worst-case $\pTwoSched$
response can be computed via dynamic programming in topological order
from the leafs to the root of $\sg$.

\mypara{MDP subroutine}
On the other hand, suppose $\pTwoSched$ was known, reducing the SG to
a MDP. As discussed in the previous section, for the MDP case, it
suffices to consider rationality indexed policies. Let use denote the
resulting MDP and maximum causal entropy family of (partial) policies
as $\sg[\pTwoSched]$ and $\pi^\pOne_\rat[\pTwoSched]$ resp. Now
suppose $\rat$ is a priori fixed. Again, via a topological ordered
evaluation of states from the leaves to the root, one can compute the
$\pTwo$ policy, call $\sched_\rat^\pTwo$, that minimizes the maximum
causal entropy policy family, $\pi^\pOne_\rat[~.~]$. By
We shall refer to resulting (partial) schedule as: $\pi_\rat \eqdef \langle
\pi^\pOne_\rat[\sched_\rat^\pTwo] , \sched_\rat^\pTwo \rangle$.

\mypara{Recursively Matching Randomness}
Of course, even if $\rat$ is fixed, $\pTwoSched$ need not be
$\sched^\pTwo_\rat$. Nevertheless, observe that by selecting the worst
entropy MDP, $\pi_\rat$ establishes an achievable randomness for the
sub-game rooted at each state, call $\delta_\rat(s)$. Now suppose,
$\pTwo$ deviates from $\sched_\rat^\pTwo$ at state $s$. Note that, so
long as $\pOneSched$ does not result yield randomness less than
$\delta_\rat(s)$, the worst-case randomness will not decrease.
Therefore, in order to determine the guaranteed performance, we
ask ``what maximum performance can one guarantee at $s' \in
\Succ(s)$ given randomness $\delta_\rat(s)$.''
\begin{mdframed}
The key observation is that each state $s'$ is the root of a sub game,
$\sg[s]$, with a corresponding Pareto Front, $\pareto{\solutions}[s]$.
\end{mdframed}
... TODO: explain dynamic programming to answer min query.




\subsection{Playing against deterministic adversaries}
To formalise our argument
\paragraph{Reformulation as a set of MDPs}

\paragraph{}


\subsection{Randomizing adversaries will not play better}

\subsection{Non-observable adversaries}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
