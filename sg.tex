
MDP algorithm in hand, we are now ready to provide an algorithm for
stochastic games.
%At a high level, this algorithm works by initially
%planning for $\pTwo$ selecting the action that minimizes randomness
%(with ties broken by performance). This assumption leads to a
%reduction to the MDP-case, where the rationality indexed family,
%$\sched_\rat$, indexes the Pareto Front.  If this assumption is ever
%violated, the resulting state must support more randomness.  The
%rationality, $\rat$, is thus lowered to match the worst case randomness,
%which due to monotonicity of $\solfuncp$, can only increase
%performance. Surprisingly, as we shall later prove, this class of
%policies indexes the Pareto Front for SGs!


\begin{figure}
\centering
\scalebox{0.8}{
\begin{tikzpicture}
    \node[sstate] (a1) {$s_0$}; 
    
	\node[astate,below=0.6cm of a1,xshift=3em] (a2) {$s_1$};
	\node[astate,below=0.6cm of a2] (a3) {$s_5$};
	
	\node[astate,below=0.6cm of a1,xshift=-3em] (a0) {$s_2$};
	\node[sstate,below=0.6cm of a0,xshift=-3em] (s1) {$s_3$};
	\node[sstate,below=0.6cm  of a0,xshift=3em] (s2) {$s_4$};
	
	\node[below=0.1cm of s1, inner sep=0.3pt] (x1) {};
	
	\node[below=0.1cm of s2, inner sep=0.3pt] (x2) {};
	
	\draw[->] (a0) -- (s1);
	\draw[->] (a0) -- (s2);
	
	\draw[dashed] (x1) -- +(1,-1) -- +(-1,-1) -- (x1);
	\draw[dashed] (x2) -- +(1,-1) -- +(-1,-1)  -- (x2);
	
	
\end{tikzpicture}
}
\caption{Simple SG to illustrate intuition {\color{red}WIP}}
\label{fig:sg:simplest}
\end{figure}
\sj{Update figure}

\mypara{Intuition}
Before we discuss the algorithm in full detail, we consider two simple cases with a single $\pTwo$-state: First, the simplest class of true 2-player SGs, with an initial $\pTwo$-state and two independent MDPs with only $\pOne$-states, as illustrated in the dashed Fig.~\ref{fig:sg:simplest} and assuming that $s_2$ is the initial state.
For both MDP subgames, we can compute (an approximation of) the Pareto front and the corresponding sets of achievable solutions as described in Section~\ref{sec:mdps}. 
The achievable points for the SG are now given by the intersection of the two points. 
However, the challenge in this generalization is to select action for $\pOne$-states.
Therefore, consider the full SG illustrated in Fig.~\ref{fig:sg:simplest} with initial state $s_0$. 
To determine $\pOneSched(s_0)$, we must plan for whatever action $\pTwo$ selects in $s_2$ and optimally without calculating all Pareto-fronts a priori. 
What our algorithm, roughly, does is to assume that in the $\pTwo$-node, $\pTwo$ aims to minimize entropy and we compute the maximal entropy of its successors ($\pOne$-states). Under this assumption, for a required randomness $\delta$ we can compute the required rationality $\rat$ to solve the overall ERCI problem. The $\pOne$-policy memorizes this rationality. Now, whenever $\pTwo$ diverges from minimizing maximal entropy, the idea is that $\pOne$ can play with higher rationality and still ensure the same randomness $\delta$. We call the increase in rationality \emph{replanning}.
By traversing the graph top-down using this idea (and memoizing the visited state-rationality pairs $s,\rat$), we can explore how we can optimize performance while always matching the required randomness.  In the remainder of this section, we formalize and analyze this algorithmic idea.

\mypara{Environment Policies}
We make some observations about the $\pTwo$-policies. 
For the purpose of ERCI, we can assume an adversarial policy that aims to disprove that we can meet the performance \emph{and} randomization requirement. 
To do so, it suffices to violate either performance \emph{or} randomization. 
Furthermore, if there is a violating $\pTwo$-policy, there is a deterministic $\pTwo$-policy that proves this. 
In particular, in every state,  $\pTwoSched$ may thus aim to disprove either objective by choosing the appropriate action, and there is no incentive to randomize.
%Finally, these responses can be computed via a standard dynamic programming (topologically from terminal states to the initial state) operation. 

%First and foremost, observe that to render an ERCI instance
%un-achievable, it suffices for $\pTwo$ to violate
%either the performance threshold \emph{or} the randomness threshold.  Next,
%notice that because $\pOneSched$ is a priori fixed, $\pTwoSched$ can
%be seen as repeatedly selecting between a convex combination of
%$\rndp$ (and $\scp$) for the corresponding sub-graph. As the maximum of a
%convex combination is always achievable on the boundaries, we can
%w.l.o.g. assume that $\pTwoSched$ is \emph{deterministic}.  Similarly,
%notice that given a fixed $\pOneSched$, the worst-case $\pTwoSched$
%response can be computed via dynamic programming in topological order
%from the leafs to the root of $\sg$.

\mypara{Counter-policies}
First, assume $\rat$ is also a priori fixed.  
Again, via dynamic programming, we can compute the
$\pTwo$ policy that minimizes the maximal causal entropy, denoted $\sched_\rat^\pTwo$.
In particular, we use the following update of \eqref{eq:mdp:v} to the SG case:
 \begin{align}
   & V_\rat(s) \eqdef  \begin{cases}
     \lambda  \cdot \indicator{s = \target} & \text{if }s \in \{ \target, \sink \},\\
     \smoothmax_{\act \in \EnAct(s)}{  Q_\rat(s,\act) } & \text{if } s \in S_\pOne. \\
     \min_{\act \in \EnAct(s)}{  Q_\rat(s,\act) } &  \text{if } s \in S_\pTwo.
   \end{cases}
 \end{align}

Second, suppose $\pTwoSched$ was fixed, than the SG reduces to 
a MDP that we denote $\sg[\pTwoSched]$.
For every $\rat$, the policy in the SG that correspond to the policy $\sched_\lambda$ in $\sg[\pTwoSched]$ is denoted $\sched^\pOne_\rat[\pTwoSched]$.
The latter (set\footnote{These policies are no longer unique because some parts of the SG may be unreachable under $\pTwoSched$} of) policies can be thought of as the counter-policies that $\pOne$ uses to maximize entropy against a fixed $\pTwoSched$. 
Together, for fixed $\rat$, we can compute the scheduler of the $\pTwo$-scheduler that minimizes maximal entropy and its counterstrategy as: $\pi_\rat \eqdef \langle
\sched^\pOne_\rat[\sched_\rat^\pTwo] , \sched_\rat^\pTwo \rangle$.
%
%On the other hand, suppose $\pTwoSched$ was known, reducing the SG to
%a MDP. As discussed in the previous section, for the MDP case, it
%suffices to consider rationality indexed policies. Let use denote the
%resulting MDP and maximum causal entropy family of (partial) policies
%as $\sg[\pTwoSched]$ and $\pi^\pOne_\rat[\pTwoSched]$ resp. Now
%suppose $\rat$ is a priori fixed. Again, via a topological ordered
%evaluation of states from the leaves to the root, one can compute the
%$\pTwo$ policy, call $\sched_\rat^\pTwo$, that minimizes the maximum
%causal entropy policy family, $\pi^\pOne_\rat[~.~]$. By
%We shall refer to resulting (partial) schedule as: $\pi_\rat \eqdef \langle
%\pi^\pOne_\rat[\sched_\rat^\pTwo] , \sched_\rat^\pTwo \rangle$.

\mypara{Replanning}
Of course, even if $\rat$ is fixed, $\pTwoSched$ need not be
$\sched^\pTwo_\rat$. Nevertheless,  by selecting the worst
entropy MDP, $\pi_\rat$ establishes an achievable randomness for the
sub-game rooted at each state, call $\delta_\rat(s)$. Now suppose,
$\pTwo$ deviates from $\sched_\rat^\pTwo$ at state $s$. Note that, so
long as $\pOneSched$  yield randomness less than
$\delta_\rat(s)$ at the new successor-state, the worst-case randomness will not decrease.
This begs the question ``what maximum performance can $\pOne$ guarantee at $s' \in
\Succ(s)$ given randomness $\delta_\rat(s)$?'' This question can be investigated on a sub-game, and eventually, must be answered on an MDP. 
\begin{mdframed}
  The key observation is that each state $s'$ is the root of a sub
  game, $\sg[s]$, with a corresponding Pareto Front,
  $\pareto{\solutions}[s]$.
\end{mdframed}
In particular, given access the characteristic functions,
$f_\solutions^{s'}$, for each $s' \in \Succ(s)$, one can compute:
\begin{equation}\label{eq:performance_lookup}
  \epsilon_\rat(s) = \min_{s'} f_\solutions^{s'}(\delta_\rat(s)).
\end{equation}
Thus, via dynamic programming, one can define $\epsilon_\rat(\iota)$.
Moreover, assuming that this entropy matching family of policies
indexes $\pareto{\solutions}$ (proved in~Sec.~\ref{sec:proofs}), one can handle
deviations from $\sigma^\pTwo_\rat$ by ``replanning''. Namely, one
extends $\pi_\rat$ at $s'$ by computing a new rationality coefficient,
$\rat'$, such that:
\begin{equation}
  \delta_{\rat'}(s') = \delta_{\rat}(s)
\end{equation}
Due to the monotonicity $\rat \leq \rat'$, and thus $\epsilon_\rat \leq \epsilon_{\rat'}$, 
Therefore, ignoring the feasibility of computing the exact Pareto
Front, we obtain a synthesis algorithm for SGs!

\mypara{Approximate Pareto Fronts}
Of course, by varying $\rat$, one can only construct approximate
Pareto fronts $\pareto{} \subseteq \pareto{\solutions}$, where we denote the downward closure of $\pareto{}$ as $\hat{\solutions} \subseteq \solutions$.
 %let $\hat{f}_\solutions^{s'}$ denote the characterising function.
    We now
adapt the above algorithm to the case where $\hat{\solutions}$ is a $\kappa$-close approximation of $\solutions$, where $\kappa$ bounds the $\infty$-norm
error. In particular, for such an approximation, the performance from state  $s$ for fixed rationality $\rat$, $\epsilon_\rat(s)$, is bounded up to  $\kappa$. 
Convex combinations of these intervals cannot
increase the error beyond $\kappa$, i.e.,
\begin{equation}
  q\cdot[x, x + \kappa] + \bar{q}\cdot[y, y + \kappa] = [z, z + \kappa],
\end{equation}
where $z = q\cdot x + \bar{q}\cdot y$. \sj{Sentences from here are vague}Thus, since $\pi_\rat$ and
$P(s, a)$ are independent of this Pareto front (and manipulate
performance via convex combinations) the error can only accumulate on
the Pareto Fronts for $s \in S_\pTwo$. Notice then that so long as,
$\kappa\cdot\tau$\sj{what is $\tau$ doing here} is enough resolution to answer $\scp_\rat <
\scthreshold$, one obtains a semi-decision procedure as in the MDP
case.\sj{We have not talked about $\scthreshold$ in this section at all...}
We propose the following high-level algorithm:
\begin{mdframed}
\begin{enumerate}
\item Let $0 < \kappa < 1$ be some arbitrary initial tolerance.
\item Recursively compute $\kappa$-close Pareto fronts for each successor state using replanning.
\item If $\scthreshold$ is within $\kappa\cdot \tau$ distance to (but outside of) $\hat{\pareto{\solutions}}$,
  halve $\kappa$ and repeat.
\end{enumerate}  
\end{mdframed}
\sj{Symmary has new symbols, and what si $\tau$ doing there?}
\mypara{Termination and Run Time}
First, the algorithm halts almost surely, that is: 
the algorithm halts if $\langle \scthreshold, \randomness \rangle$ is
not on $\pareto{\solutions}$ (or if we happen to exactly hit $\langle \scthreshold, \randomness\rangle$ by selecting some rationality $\rat$).
As the Pareto Front has
measure 0, we argue that not halting is thus merely a technical concern, as a
small perturbation to the ERCI instance (i.e. a \emph{smoothed
analysis}~\cite{SmoothedAnalysis}) on $\sg$ admits decidability. 
The same observation hold also for the MDP case.

While for practical run time, the algorithm can be significantly improved by adaptive
tolerances, lazily computing the Pareto Fronts, and only computing
Pareto Fronts for $\pTwo$ states.
We give an output-sensitive analysis of the run time (assuming it does halt) in the naive formulation.
 If $\kappa^*$ tolerance is required to terminate,
then the $\kappa$ search introduces $\mathcal{O}(\log(\nicefrac{1}{\kappa^*}))$
iterations. Furthermore, by computing $\mathcal{O}(|\sg|)$ Pareto fronts, one ensures that the complexity grows linearly with the graph
size - although the multiplicative constant depends on the number of
rationality coefficients explored per Pareto front. We found that most
of the rationality coefficients explored were shared, which in
practice seems to amortize the cost per state. 

%In practice, this algorithm can be significantly improved by adaptive
%tolerances, lazily computing the Pareto Fronts, and only computing
%Pareto Fronts for $\pTwo$ states. Nevertheless,
%already this na\"ive algorithm gives a sense of the run-time
%bottlenecks. Namely, if $\kappa^*$ tolerance is required to terminate,
%then the $\kappa$ search introduces $O(\log(\nicefrac{1}{\kappa^*}))$
%iterations. Furthermore, by computing $O(|\sg|)$ Pareto fronts, from the
%leaves, one ensures that the complexity grows linearly with the graph
%size - although the multiplicative constant depends on the number of
%rationality coefficients explored per Pareto front. We found that most
%of the rationality coefficients explored were shared, which in
%practice seems to amortize the cost per state. Finally, as in the
%MDP-case, the algorithm halts if $\langle \scthreshold, \randomness \rangle$ is
%bounded away from the root Pareto Front. As the Pareto Front has
%measure 0, we argue that this is merely a technical concern, as a
%small perturbation to the ERCI instance (i.e. a Smoothed
%Analysis~\cite{SmoothedAnalysis}) on $\sg$ admits decidability.

\mypara{On Completeness}
Our algorithm restricts itself to considering only recursive entropy
matching policies, $\{\sched^{\pOne}_\rat\}_\rat$.  Importantly,
observe that because fixing a policy yields a verifiable point in
$\solutions$, any witness for satisfiability we find is trivially sound. We can restrict ourselves to the case in which our
algorithm claims the ERCI instance unsatisfiable. Surprisingly, the class of policies we consider suffices, and the algorithm 
is thus sound and (whenever halting) complete (proof provided in Sec~\ref{sec:proofs}).

Finally, observe that as a corollary of the entropy matching family
$\{\sched^{\pOne}_\rat\}_\rat$ being complete, it must be the case that
$\solfuncp(\rndp_\rat)$ inherits continuity and (strict) monotonicity
from the MDP case. Namely, at each $\pTwo$ state, the achievable
points $\solutions$ are necessarily the intersection of the achievable
points of the subgraphs. By induction, (with the MDP base case), we
obtain continuity and strict monotonicity.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
