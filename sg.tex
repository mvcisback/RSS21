
MDP algorithm in hand, we are now ready to provide an algorithm for
stochastic games.
%At a high level, this algorithm works by initially
%planning for $\pTwo$ selecting the action that minimizes randomness
%(with ties broken by performance). This assumption leads to a
%reduction to the MDP-case, where the rationality indexed family,
%$\sched_\rat$, indexes the Pareto front.  If this assumption is ever
%violated, the resulting state must support more randomness.  The
%rationality, $\rat$, is thus lowered to match the worst case randomness,
%which due to monotonicity of $\solfuncp$, can only increase
%performance. Surprisingly, as we shall later prove, this class of
%policies indexes the Pareto front for SGs!


\mypara{Environment Policies} We begin with three observations about
the $\pTwo$-policies.  First, for ERCI, we can assume an adversary for $\pTwo$ 
that aims to foil $\pOne$ achieving both the performance \emph{and}
randomization requirement. We call such a $\pTwo$-policy \emph{violating.} For a policy to be violating,  suffices to violate, against every $\pOne$-policy independently, either
performance \emph{or} randomization.  Second, if there is a violating
$\pTwo$-policy, there is a deterministic $\pTwo$-policy that proves
this.  In particular, at every state, $\pTwoSched$ may choose to
violate either constraint via the appropriate action with no incentive
to randomize. Third, fixing an environment policy reduces $\sg$ to a
MDP $\sg[\pTwoSched]$.

\begin{figure}[t]
\centering
\scalebox{0.8}{
\input{sg_vis.tex}
}
\caption{SG to illustrate entropy matching policies.}
\label{fig:sg:simplest}
\end{figure}

\mypara{A Sufficient Class of Policies}
% Before we discuss the algorithm in full detail, we consider two simple
% cases with a single $\pTwo$-state \footnote{To ensure we satisfy our
% our assumption of games being alternating between both players, we can
% insert dummy states.} and multiple $\pOne$-states.  First, we consider
% a SG with an initial $\pTwo$-state, $s_2$, that can transition to two
% independent MDPs (a sub-game with only $\pOne$-states) as illustrated
% in the dashed box in Fig.~\ref{fig:sg:simplest}.  Here, $\pTwo$ can
% force $\pOne$ to play in either of two MDPs that start in states $s_3$
% and $s_4$, respectively.  For both MDPs, we can compute (an
% approximation of) the Pareto front and the corresponding sets of
% achievable points as described in Sec.~\ref{sec:mdps}.  The achievable
% points for state $s_2$ are now given by the intersection of the two
% sets.
% % However, the challenge in this generalization is to select
% % action for $\pOne$-states.
% Now, consider the full SG illustrated in Fig.~\ref{fig:sg:simplest}
% (initial state $s_0$).  To determine $\pOneSched(s_0)$, we must plan
% for whatever action $\pTwo$ selects in $s_2$. Thus, the Pareto front for
% $s_0$ non-trivially depends on the Pareto curves for $s_2$ and
% $s_1$. 
% Algorithmically, we want to avoid computing the (complete)
% Pareto fronts as much as possible. To do so, we restrict our attention
% to policies induced by some rationality $\rat$ (as in the MDP case).
% Later we prove this restriction is w.l.o.g. Conceptually,
% $\rat$ then enables $\pOne$ to associate ``comparable'' points between
% sibling Pareto fronts, and thus reason locally.
For MDPs, we have seen that varying rationality is sufficient to explore the Pareto curve. 
We show that we can adapt that idea to a class we call \emph{entropy matching policies}, which may be indexed by the (initial) rationality. 
In the initial state, we start by assuming that $\pTwo$
selects a (deterministic) policy, $\sched^\rat_\pTwo$, that
lexicographically minimizes the guaranteed randomness, followed by
performance. On the sub-graph, $\sg[\sched^\rat_\pTwo]$, $\pOne$
employs the corresponding entropy maximizing policy for the MDP $\sg[\sched^\rat_\pTwo]$. 
Whenever $\pTwo$ diverges from the entropy minimizing policy (to decrease the induced performance), we let $\pOne$ increase its rationality such that it still induces the same guaranteed randomness.  We refer to this idea as \emph{entropy matching}. The idea is that the rationality at the initial state induces a worst-case entropy, and whatever $\pTwo$ chooses to do, throughout the SG, we ensure that we indeed obtain this entropy. 
The policy thus tracks this entropy and if necessary adapts the rationality (which we call \emph{replanning}). 
Replanning ensures we obtain the optimal performance from a particular point while still ensuring the required randomness.

\begin{example}
We sketch an entropy matching policy in Fig.~\ref{fig:sg:simplest}. In particular, we show part of SG. For some fixed rationality $\lambda$, we annotate in red, on the left of the SG states, the entropy obtained when assuming that $\pTwo$ plays an entropy-minimizing policy as outlined above. In particular, this means that in $s_2$, $\pTwo$ selects action $a$. Now, our entropy-matching policy (in blue, on the right) will play with entropy $\lambda$, unless state $s_4$ is reached. As this ensures a higher entropy, we may now select a higher rationality.
\end{example}

%
%Our algorithm operates by picking an optimization direction
%$\langle \rat, 1 \rangle$ and \emph{temporarily} assuming that $\pTwo$
%selects a (deterministic) policy, $\sched^\rat_\pTwo$, that
%lexicographically minimizes the guaranteed randomness, followed by
%performance. On the sub-graph, $\sg[\sched^\rat_\pTwo]$, $\pOne$
%employs the corresponding entropy maximizing policy for the MDP,
%$\sg[\sched^\rat_\pTwo]$. This partial-policy is extended to a policy
%on the rest of the $\sg$ as follows. Whenever $\pTwo$ diverges from
%the entropy minimizing policy, it must be possible for $\pOne$ to
%trade randomness for performance, i.e. increase $\rat$, while still
%ensuring the same randomness against a entropy minimizing
%adversary.\sj{This needs at least one more sentence} Thus, a new MDP partial-policy\sj{???} is computed and extended
%recursively to all states of the SG.  We call the increase
%in rationality (and the associated computations) \emph{replanning} and
%the corresponding family of policies, $\{\sched^\rat_\pOne\}_\rat$,
%\emph{entropy matching}.  Finally, 

\mypara{Soundness and Completeness}
Importantly, observe that because
fixing a policy for $\pOne$ yields a verifiable point in $\solutions$, any witness
for realizability we find is trivially sound. For completeness, we can
restrict ourselves to the case in which our algorithm claims the ERCI
instance unrealizable. Surprisingly, the class of policies we consider
suffices, and the algorithm is thus sound and (whenever halting)
complete (proof provided in Sec~\ref{sec:proofs}). That is, all
guaranteed points are witnessed by an entropy matching policy!

Further, observe that as a corollary of the entropy matching family
being complete, it must be the case that $\solfuncp(\rndp_\rat)$
inherits continuity and (strict) monotonicity from the MDP
case. Namely, at each $\pTwo$ state, the achievable points
$\solutions$ are necessarily the intersection of the achievable points
of the sub-graphs. By induction, (with the MDP base case), we obtain
continuity and strict monotonicity.


\mypara{Algorithm: Memoizing Pareto Fronts}
We propose approximating the  Pareto front using the same three staged sequence of exploring 
rationality coefficients (at the initial state) as the MDP case: (1) endpoints, (2) doubling,
(3) binary search.

To perform the above computations efficiently, we adopt a geometric
perspective. Namely, observe that each node of $\sg$ indexes a
sub-graph, which has a corresponding Pareto front for trading
performance for randomness. Further, note that the Pareto front at an
$\pTwo$ node is the intersection of the Pareto fronts of its child
nodes. Entropy matching thus corresponds to ``jumping'' between Pareto
fronts and adjusting the optimization direction by increasing the
rationality.  Thus, by traversing the graph from the terminal states
to the initial state, approximating Pareto fronts along the way, one
can memoize how to trade performance for randomness at any given
node. This preprocessing enables determining the minimum entropy
response for any optimization direction and quickly replanning via a
convex combination of Pareto optimal policies.


%Finally, these responses can be computed via a standard dynamic programming (topologically from terminal states to the initial state) operation. 

%First and foremost, observe that to render an ERCI instance
%un-achievable, it suffices for $\pTwo$ to violate
%either the performance threshold \emph{or} the randomness threshold.  Next,
%notice that because $\pOneSched$ is a priori fixed, $\pTwoSched$ can
%be seen as repeatedly selecting between a convex combination of
%$\rndp$ (and $\scp$) for the corresponding sub-graph. As the maximum of a
%convex combination is always achievable on the boundaries, we can
%w.l.o.g. assume that $\pTwoSched$ is \emph{deterministic}.  Similarly,
%notice that given a fixed $\pOneSched$, the worst-case $\pTwoSched$
%response can be computed via dynamic programming in topological order
%from the leafs to the root of $\sg$.


% \mypara{Minimum Entropy Sub-graphs} Next, we formalize the
% environment's entropy minimizing counter-policy.  First, assume $\rat$
% is a priori fixed. Then, note that the minimum entropy
% counter-strategy to $\rat$, call $\sched_\pTwo^\rat$, and the
% corresponding $\pOne$ partial\footnotemark policy,
% $\sched_\pOne^\rat$, can be computed from the sinks to root as
% follows.
% \begin{enumerate}
% \item \textbf{Sink}:
%   No actions, thus $\sched^\rat = \langle \sched_\pOne^\rat, \sched_\pTwo^\rat\rangle$
%   is trivial.
% \item \textbf{$\pTwo$-node}: Suppose $\sched^\rat$ is known for the
%   sub-graphs accessed by each action. By assumption,
%   $\sched_\pTwo^\rat$ will pick the entropy minimizing action (with
%   ties broken by performance).
% \item \textbf{$\pOne$-node}: suppose $\sched_\pTwo^\rat$ is fixed,
%   reducing the SG to a MDP, with corresponding policy
%   $\sched_\pOne^\rat$. Importantly, because of the uniqueness of
%   $\sched_\pOne^\rat$ on MDPs, $\pOne$'s policy remains unchanged on
%   all sub-graphs accessed by its actions. Thus $\sched_\pTwo^\rat$
%   remains unchanged.


% \end{enumerate}
%   \footnotetext{These policies are no longer unique because some
%   parts of the SG may be unreachable under $\sched_\pTwo^\rat$}
% %
%On the other hand, suppose $\pTwoSched$ was known, reducing the SG to
%a MDP. As discussed in the previous section, for the MDP case, it
%suffices to consider rationality indexed policies. Let use denote the
%resulting MDP and maximum causal entropy family of (partial) policies
%as $\sg[\pTwoSched]$ and $\pi^\pOne_\rat[\pTwoSched]$ resp. Now
%suppose $\rat$ is a priori fixed. Again, via a topological ordered
%evaluation of states from the leaves to the root, one can compute the
%$\pTwo$ policy, call $\sched_\rat^\pTwo$, that minimizes the maximum
%causal entropy policy family, $\pi^\pOne_\rat[~.~]$. By
%We shall refer to resulting (partial) schedule as: $\pi_\rat \eqdef \langle
%\pi^\pOne_\rat[\sched_\rat^\pTwo] , \sched_\rat^\pTwo \rangle$.

% \mypara{Replanning}
% Of course, even if $\rat$ is fixed, $\pTwoSched$ need not be
% $\sched^\pTwo_\rat$. Nevertheless,  by selecting the worst
% entropy MDP, $\sched_\rat$ establishes an achievable randomness for the
% sub-game rooted at each state, call $\rndp_\rat(s)$. Now suppose,
% $\pTwo$ deviates from $\sched_\rat^\pTwo$ at state $s$. Note that, so
% long as $\pOneSched$  yield randomness less than
% $\rndp_\rat(s)$ at the new successor-state, the worst-case randomness will not decrease.
% This begs the question ``what maximum performance can $\pOne$ guarantee at $s' \in
% \Succ(s)$ given randomness $\rndp_\rat(s)$?'' This question can be investigated on a sub-game, and eventually, must be answered on an MDP. 
% \begin{mdframed}
%   The key observation is that each state $s'$ is the root of a sub
%   game, $\sg[s]$, with a corresponding Pareto front,
%   $\pareto{\solutions}[s]$.
% \end{mdframed}
% In particular, given access the characteristic functions,
% $f_\solutions^{s'}$, for each $s' \in \Succ(s)$, one can compute:
% \begin{equation}\label{eq:performance_lookup}
%   \epsilon_\rat(s) = \min_{s'} f_\solutions^{s'}(\delta_\rat(s)).
% \end{equation}
% Thus, via dynamic programming, one can define $\epsilon_\rat(\iota)$.
% Moreover, assuming that this entropy matching family of policies
% indexes $\pareto{\solutions}$ (proved in~Sec.~\ref{sec:proofs}), one can handle
% deviations from $\sigma^\pTwo_\rat$ by ``replanning''. Namely, one
% extends $\pi_\rat$ at $s'$ by computing a new rationality coefficient,
% $\rat'$, such that:
% \begin{equation}
%   \delta_{\rat'}(s') = \delta_{\rat}(s)
% \end{equation}
% Due to the monotonicity $\rat \leq \rat'$, and thus $\epsilon_\rat \leq \epsilon_{\rat'}$, 
% Therefore, ignoring the feasibility of computing the exact Pareto
% Front, we obtain a synthesis algorithm for SGs!
\mypara{Approximate Pareto Fronts}
Of course, by varying $\rat$, one can only construct approximate
Pareto fronts $\hat{\pareto{}} \subseteq \pareto{\solutions}$, where we denote the downward closure of $\hat{\pareto{}}$ as $\hat{\solutions} \subseteq \solutions$.
% let $\hat{f}_\solutions^{s'}$ denote the characterising function.
We propose the following high-level algorithm to adapt the above
algorithm to the case where each Pareto front approximation introduces at
most $\kappa$ error along the performance axis.
\begin{mdframed}
\begin{enumerate}
\item Let $\tau$ denote the length of the longest path in $\sg$.
\item Let $0 < \kappa < 1$ be some arbitrary initial tolerance.
\item Recursively compute $\kappa$-close Pareto fronts for each successor state using replanning.
\item If the any minimum entropy action cannot be determined or $\scthreshold$ is within $\kappa\cdot \tau$ distance to (but outside of) $\hat{\pareto{}}$,
  halve $\kappa$ and repeat.
\item Otherwise, perform entropy matching algorithm (with initial
  entropy $\randomness$) using these Pareto fronts and return the
  resulting policy (if on exists).
\end{enumerate}  
\end{mdframed}
The soundness of this algorithm relies on the following critical
facts: (1) Given sufficient resolution, the minimum entropy
$\pTwo$-actions can be determined and the resulting entropy. (2) The
resulting entropy is depends solely on
the resulting sub-graph (and is independent of the current Pareto
approximation). (3) Thus, when querying points on
$\pareto{\solutions}$, error can only accumulate for $\scp$. (4) Next,
observe that $\scp$ is computed using convex combinations of entropy
matched points on Pareto approximations. (5) Convex combinations of an error interval cannot
increase the error, i.e.,
\begin{equation}
  q\cdot[x, x + \kappa] + \bar{q}\cdot[y, y + \kappa] = [z, z + \kappa],
\end{equation}
where $z = q\cdot x + \bar{q}\cdot y$.
Thus, so long as $\kappa\cdot\tau$ is enough resolution to answer $\scp_\rat <
\scthreshold$, one obtains a semi-decision procedure as in the MDP
case.

\mypara{Termination and Run Time} First, as in the MDP case, the
algorithm terminates almost surely, with the exception occurring only
for a subset of the Pareto front.  Below, we give an output-sensitive
analysis of the run time (assuming it does halt).  If $\kappa^*$
tolerance is required to terminate, then the $\kappa$ search
introduces $\mathcal{O}(\log(\nicefrac{1}{\kappa^*}))$
iterations. Next, observe that each node processes a given rationality
coefficient at most once. Further, looking up which pair of rationalities
are need to upper and lower bound the performance for a given randomness
can be in logarithmic time via binary search on rationality coefficients.
As the corresponding bounds and convex combinations can be computed in
constant time, this means this algorithm runs in time:
\begin{equation}
  \label{eq:sg-runtime}
  \mathcal{O}\Big(\log(\nicefrac{1}{\kappa^*})\cdot N_\rat\cdot \log(N_\rat)\cdot |\sg|\Big),
\end{equation}
where, $N_\rat$ is the number of unique rationality coefficients
processed.  If, as in the MDP case, one assumes a maximum rationality
coefficient $\rat^*$ and a minimum rationality resolution $\Delta$,
one obtains:
\begin{equation}
  \mathcal{O}\Big(\underbrace{\log(\nicefrac{1}{\kappa^*})}_{\kappa \text{ search}}\cdot \underbrace{\nicefrac{\rat^*}{\Delta}\cdot \overbrace{\log(\nicefrac{\rat^*}{\Delta})}^{\text{Replanning}}\cdot |\sg|}_{\text{Evaluate } \rat}\Big).
\end{equation}
The above however is very conservative and empirically we observe
$N_\rat$ bounded far away from $\nicefrac{\rat^*}{\Delta}$.

%In practice, this algorithm can be significantly improved by adaptive
%tolerances, lazily computing the Pareto Fronts, and only computing
%Pareto Fronts for $\pTwo$ states. Nevertheless,
%already this na\"ive algorithm gives a sense of the run-time
%bottlenecks. Namely, if $\kappa^*$ tolerance is required to terminate,
%then the $\kappa$ search introduces $O(\log(\nicefrac{1}{\kappa^*}))$
%iterations. Furthermore, by computing $O(|\sg|)$ Pareto fronts, from the
%leaves, one ensures that the complexity grows linearly with the graph
%size - although the multiplicative constant depends on the number of
%rationality coefficients explored per Pareto front. We found that most
%of the rationality coefficients explored were shared, which in
%practice seems to amortize the cost per state. Finally, as in the
%MDP-case, the algorithm halts if $\langle \scthreshold, \randomness \rangle$ is
%bounded away from the root Pareto Front. As the Pareto Front has
%measure 0, we argue that this is merely a technical concern, as a
%small perturbation to the ERCI instance (i.e. a Smoothed
%Analysis~\cite{SmoothedAnalysis}) on $\sg$ admits decidability.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
