
MDP algorithm in hand, we are now ready to provide an algorithm for
stochastic games.
%At a high level, this algorithm works by initially
%planning for $\pTwo$ selecting the action that minimizes randomness
%(with ties broken by performance). This assumption leads to a
%reduction to the MDP-case, where the rationality indexed family,
%$\sched_\rat$, indexes the Pareto Front.  If this assumption is ever
%violated, the resulting state must support more randomness.  The
%rationality, $\rat$, is thus lowered to match the worst case randomness,
%which due to monotonicity of $\solfuncp$, can only increase
%performance. Surprisingly, as we shall later prove, this class of
%policies indexes the Pareto Front for SGs!


\todo[inline]{Add bellman backup direction and pareto front replanning viz to Fig 5.}

\mypara{Algorithmic Idea}
% Before we discuss the algorithm in full detail, we consider two simple
% cases with a single $\pTwo$-state \footnote{To ensure we satisfy our
% our assumption of games being alternating between both players, we can
% insert dummy states.} and multiple $\pOne$-states.  First, we consider
% a SG with an initial $\pTwo$-state, $s_2$, that can transition to two
% independent MDPs (a sub-game with only $\pOne$-states) as illustrated
% in the dashed box in Fig.~\ref{fig:sg:simplest}.  Here, $\pTwo$ can
% force $\pOne$ to play in either of two MDPs that start in states $s_3$
% and $s_4$, respectively.  For both MDPs, we can compute (an
% approximation of) the Pareto front and the corresponding sets of
% achievable points as described in Sec.~\ref{sec:mdps}.  The achievable
% points for state $s_2$ are now given by the intersection of the two
% sets.
% % However, the challenge in this generalization is to select
% % action for $\pOne$-states.
% Now, consider the full SG illustrated in Fig.~\ref{fig:sg:simplest}
% (initial state $s_0$).  To determine $\pOneSched(s_0)$, we must plan
% for whatever action $\pTwo$ selects in $s_2$. Thus, the Pareto front for
% $s_0$ non-trivially depends on the Pareto curves for $s_2$ and
% $s_1$. 
% Algorithmically, we want to avoid computing the (complete)
% Pareto fronts as much as possible. To do so, we restrict our attention
% to policies induced by some rationality $\rat$ (as in the MDP case).
% Later we prove this restriction is w.l.o.g. Conceptually,
% $\rat$ then enables $\pOne$ to associate ``comparable'' points between
% sibling Pareto fronts, and thus reason locally.
Our algorithm operates by picking an optimization direction
$\langle \rat, 1 \rangle$ and \emph{temporarily} assuming that $\pTwo$ aims
to minimize the maximal causal entropy of $\pOne$. Applying this assumption recursively, one collapses the SG to an MDP.

Now suppose, we can finds a $\pOne$-policy that optimizes for
direction $\langle \rat, 1\rangle$ with sufficient randomness and
performance against its minimum entropy response.  Next, observe that
because optimal performance decreases given randomness, whenever
$\pTwo$ diverges from minimizing maximal entropy, it must be possible
to increase the rationality $\rat$ of the policy, thus increasing the
performances, while still ensuring the same randomness!

We call the increase in rationality (and the associated computations)
\emph{replanning} and the induced family of policies \emph{entropy
  matching}.  By traversing the graph from the sinks to the source,
approximating Pareto Fronts along the way, we can explore how we can
optimize performance while always matching the required randomness.
In the remainder of this section, we formalize and analyze this
algorithmic idea and prove soundness and completeness of entropy
matching.


\mypara{Environment Policies} Let us begin by making two observations
about the $\pTwo$-policies.  First, for ERCI, we can assume an
adversary that aims to foil $\pOne$ achieving both the performance
\emph{and} randomization requirement.  To do so, it suffices to
violate either performance \emph{or} randomization.  Second, if there
is a violating $\pTwo$-policy, there is a deterministic $\pTwo$-policy
that proves this.  In particular, at every state, $\pTwoSched$ may
choose to violate either constraint via the appropriate
action with no incentive to randomize.
%Finally, these responses can be computed via a standard dynamic programming (topologically from terminal states to the initial state) operation. 

%First and foremost, observe that to render an ERCI instance
%un-achievable, it suffices for $\pTwo$ to violate
%either the performance threshold \emph{or} the randomness threshold.  Next,
%notice that because $\pOneSched$ is a priori fixed, $\pTwoSched$ can
%be seen as repeatedly selecting between a convex combination of
%$\rndp$ (and $\scp$) for the corresponding sub-graph. As the maximum of a
%convex combination is always achievable on the boundaries, we can
%w.l.o.g. assume that $\pTwoSched$ is \emph{deterministic}.  Similarly,
%notice that given a fixed $\pOneSched$, the worst-case $\pTwoSched$
%response can be computed via dynamic programming in topological order
%from the leafs to the root of $\sg$.
\begin{figure}[h]
\centering
\scalebox{0.8}{
\input{sg_vis.tex}
}
\caption{Simple SG to illustrate intuition}
\label{fig:sg:simplest}
\end{figure}

\mypara{Minimum Entropy Sub-graphs} Next, we formalize the
environment's entropy minimizing counter-policy.  First, assume $\rat$
is a priori fixed.  Next, note that the minimum entropy
counter-strategy to $\rat$, call $\sched_\pTwo^\rat$, and the
corresponding $\pOne$ partial\footnotemark policy,
$\sched_\pOne^\rat$, can be computed from the sinks to root as
follows. 1. There are no actions at a sink, so
$\sched^\rat = \langle \sched_\pOne^\rat, \sched_\pTwo^\rat\rangle$ is
trivial. 2. At a $\pTwo$-node, suppose $\sched^\rat$ is known for the
sub-graphs accessed by each action. By assumption, $\sched_\pTwo^\rat$
will pick the entropy minimizing action (with ties broken by
performance). 3. At a $\pOne$-node, suppose $\sched_\pTwo^\rat$ is
fixed, reducing the SG to a MDP, with corresponding policy
$\sched_\pOne^\rat$.
\begin{remark}
  Importantly, because of the uniqueness of $\sched_\pOne^\rat$ on
  MDPs, $\pOne$'s policy remains unchanged on all subgraphs accessed
  by its actions. Thus $\sched_\pTwo^\rat$ remains unchanged.
\end{remark}
\footnotetext{These policies are no longer unique because some
  parts of the SG may be unreachable under $\sched_\pTwo^\rat$}
%
%On the other hand, suppose $\pTwoSched$ was known, reducing the SG to
%a MDP. As discussed in the previous section, for the MDP case, it
%suffices to consider rationality indexed policies. Let use denote the
%resulting MDP and maximum causal entropy family of (partial) policies
%as $\sg[\pTwoSched]$ and $\pi^\pOne_\rat[\pTwoSched]$ resp. Now
%suppose $\rat$ is a priori fixed. Again, via a topological ordered
%evaluation of states from the leaves to the root, one can compute the
%$\pTwo$ policy, call $\sched_\rat^\pTwo$, that minimizes the maximum
%causal entropy policy family, $\pi^\pOne_\rat[~.~]$. By
%We shall refer to resulting (partial) schedule as: $\pi_\rat \eqdef \langle
%\pi^\pOne_\rat[\sched_\rat^\pTwo] , \sched_\rat^\pTwo \rangle$.

\mypara{Replanning}
Of course, even if $\rat$ is fixed, $\pTwoSched$ need not be
$\sched^\pTwo_\rat$. Nevertheless,  by selecting the worst
entropy MDP, $\sched_\rat$ establishes an achievable randomness for the
sub-game rooted at each state, call $\rndp_\rat(s)$. Now suppose,
$\pTwo$ deviates from $\sched_\rat^\pTwo$ at state $s$. Note that, so
long as $\pOneSched$  yield randomness less than
$\rndp_\rat(s)$ at the new successor-state, the worst-case randomness will not decrease.
This begs the question ``what maximum performance can $\pOne$ guarantee at $s' \in
\Succ(s)$ given randomness $\rndp_\rat(s)$?'' This question can be investigated on a sub-game, and eventually, must be answered on an MDP. 
\begin{mdframed}
  The key observation is that each state $s'$ is the root of a sub
  game, $\sg[s]$, with a corresponding Pareto Front,
  $\pareto{\solutions}[s]$.
\end{mdframed}
In particular, given access the characteristic functions,
$f_\solutions^{s'}$, for each $s' \in \Succ(s)$, one can compute:
\begin{equation}\label{eq:performance_lookup}
  \epsilon_\rat(s) = \min_{s'} f_\solutions^{s'}(\delta_\rat(s)).
\end{equation}
Thus, via dynamic programming, one can define $\epsilon_\rat(\iota)$.
Moreover, assuming that this entropy matching family of policies
indexes $\pareto{\solutions}$ (proved in~Sec.~\ref{sec:proofs}), one can handle
deviations from $\sigma^\pTwo_\rat$ by ``replanning''. Namely, one
extends $\pi_\rat$ at $s'$ by computing a new rationality coefficient,
$\rat'$, such that:
\begin{equation}
  \delta_{\rat'}(s') = \delta_{\rat}(s)
\end{equation}
Due to the monotonicity $\rat \leq \rat'$, and thus $\epsilon_\rat \leq \epsilon_{\rat'}$, 
Therefore, ignoring the feasibility of computing the exact Pareto
Front, we obtain a synthesis algorithm for SGs!

\mypara{Approximate Pareto Fronts}
Of course, by varying $\rat$, one can only construct approximate
Pareto fronts $\hat{\pareto{}} \subseteq \pareto{\solutions}$, where we denote the downward closure of $\hat{\pareto{}}$ as $\hat{\solutions} \subseteq \solutions$.
% let $\hat{f}_\solutions^{s'}$ denote the characterising function.
We propose the following high-level algorithm to
adapt the above algorithm to the case where
$\hat{\solutions}$ is a $\kappa$-close approximation of $\solutions$,
for $\kappa$ bounding the $\infty$-norm error. In particular, the
performance from state $s$ for fixed rationality,
$\epsilon_\rat(s)$, is bounded up to $\kappa$.
\begin{mdframed}
\begin{enumerate}
\item Let $\tau$ denote the length of the longest path in $\sg$.
\item Let $0 < \kappa < 1$ be some arbitrary initial tolerance.
\item Recursively compute $\kappa$-close Pareto fronts for each successor state using replanning.
\item If $\scthreshold$ is within $\kappa\cdot \tau$ distance to (but outside of) $\hat{\pareto{}}$,
  halve $\kappa$ and repeat.
\end{enumerate}  
\end{mdframed}
The soundness of this algorithm relies on the following critical
facts: (1) Each rationality coefficient induces an entropy based on
the subgraph (and is independent of the current Pareto
approximation). (2) Thus, when querying points on
$\pareto{\solutions}$, error can only accumulate for $\scp$. (3) Next,
observe that $\scp$ is computed using convex combinations of entropy
matched points on Pareto approximations. (4) Convex combinations of an error interval cannot
increase the error, i.e.,
\begin{equation}
  q\cdot[x, x + \kappa] + \bar{q}\cdot[y, y + \kappa] = [z, z + \kappa],
\end{equation}
where $z = q\cdot x + \bar{q}\cdot y$.
Thus, so long as $\kappa\cdot\tau$ is enough resolution to answer $\scp_\rat <
\scthreshold$, one obtains a semi-decision procedure as in the MDP
case.

\mypara{Termination and Run Time}
First, as in the MDP case, the algorithm terminates almost surely.
While for practical run time, the algorithm can be significantly improved by adaptive
tolerances, lazily computing the Pareto Fronts, and only computing
Pareto Fronts for $\pTwo$ states.
We give an output-sensitive analysis of the run time (assuming it does halt) in the naive formulation.
 If $\kappa^*$ tolerance is required to terminate,
then the $\kappa$ search introduces $\mathcal{O}(\log(\nicefrac{1}{\kappa^*}))$
iterations. Furthermore, by computing $\mathcal{O}(|\sg|)$ Pareto fronts, one ensures that the complexity grows linearly with the graph
size - although the multiplicative constant depends on the number of
rationality coefficients explored per Pareto front. We found that most
of the rationality coefficients explored were shared, which in
practice seems to amortize the cost per state. 

%In practice, this algorithm can be significantly improved by adaptive
%tolerances, lazily computing the Pareto Fronts, and only computing
%Pareto Fronts for $\pTwo$ states. Nevertheless,
%already this na\"ive algorithm gives a sense of the run-time
%bottlenecks. Namely, if $\kappa^*$ tolerance is required to terminate,
%then the $\kappa$ search introduces $O(\log(\nicefrac{1}{\kappa^*}))$
%iterations. Furthermore, by computing $O(|\sg|)$ Pareto fronts, from the
%leaves, one ensures that the complexity grows linearly with the graph
%size - although the multiplicative constant depends on the number of
%rationality coefficients explored per Pareto front. We found that most
%of the rationality coefficients explored were shared, which in
%practice seems to amortize the cost per state. Finally, as in the
%MDP-case, the algorithm halts if $\langle \scthreshold, \randomness \rangle$ is
%bounded away from the root Pareto Front. As the Pareto Front has
%measure 0, we argue that this is merely a technical concern, as a
%small perturbation to the ERCI instance (i.e. a Smoothed
%Analysis~\cite{SmoothedAnalysis}) on $\sg$ admits decidability.

\mypara{On Completeness}
Our algorithm restricts itself to considering only recursive entropy
matching policies, $\{\sched^{\pOne}_\rat\}_\rat$.  Importantly,
observe that because fixing a policy yields a verifiable point in
$\solutions$, any witness for realizability we find is trivially sound. For completeness, we can restrict ourselves to the case in which our
algorithm claims the ERCI instance unrealizable. Surprisingly, the class of policies we consider suffices, and the algorithm 
is thus sound and (whenever halting) complete (proof provided in Sec~\ref{sec:proofs}).

Finally, observe that as a corollary of the entropy matching family
$\{\sched^{\pOne}_\rat\}_\rat$ being complete, it must be the case that
$\solfuncp(\rndp_\rat)$ inherits continuity and (strict) monotonicity
from the MDP case. Namely, at each $\pTwo$ state, the achievable
points $\solutions$ are necessarily the intersection of the achievable
points of the subgraphs. By induction, (with the MDP base case), we
obtain continuity and strict monotonicity.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
