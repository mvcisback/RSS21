\subsection{Stochastic Games}
An (2.5-player) \emph{stochastic game} (SG) is a tuple $\sg = \langle S, \iota, \Act, P \rangle$. The set of \emph{states} $S = S_1 \cup S_2$ is partitioned into a set $S_1$ of player-1 states and a set $S_2$ of player-2 states. $\iota \in S_1$ is the \emph{initial state}, $\Act = \Act_1 \cup \Act_2$ is a finite set of \emph{actions}, and $P\colon S \times \Act \rightarrow \Distr(S)$ is the \emph{transition function} defined by two transition functions: $P_1\colon S_1 \times \Act_1 \rightarrow \Distr(S)$, $P_2\colon S_2 \times \Act_2 \rightarrow \Distr(S)$.\sj{Do we assume each action exists in every state? It makes the defionitions nicer..} 
A finite \emph{path} $\pi$ of length $n$ is a sequence $(\iota = s_0 ) \xrightarrow{\act_0} s_1 \xrightarrow{\act_1} s_2 \rightarrow \hdots \rightarrow s_n$ in $\left( S \times \Act \right)^{n} \times S$. We denote the length with $|\pi|$, and denote $s_n$, i.e., the last element of $\pi$ with $\last{\pi}$. The set of all finite paths is denoted $\Paths[n]{\sg}$, and $\Paths{\sg} = \bigcup_{n \in \NN} \Paths[n]{\sg}$. We omit $\sg$ whenever it is clear from the context. It is helpful to partition paths based on their last state: $\POnePaths[n]{\sg} = \{ \pi \in \Paths[n]{\sg} \mid \last{\pi} \in S_1 \}$ and $\PTwoPaths[n]{} = \Paths[n]{} \setminus \POnePaths[n]{}$.

An SG is finite, if its state space is finite.
If $\Act_2$ is a singleton set, then $\sg$ is an \emph{Markov decision process}.
If both $\Act_1$ and $\Act_2$ are singleton sets, then $\sg$ is a \emph{Markov chain}. For a Markov chain $\mc = \langle S, \iota, P \rangle$, we omit $\Act$ and simplify the signature of $P$ to $P\colon S\rightarrow \Distr(S)$.
If $P(s,\act)$ is a Dirac distribution for every $s \in S$ and $\act \in \Act$, then $\sg$ is called \emph{deterministic}.


%\paragraph{Policies.} 
As standard, before we can define probabilities, all nondeterminism needs to be resolved. We do this with the notion of a policy. A \emph{policy} is a tuple $\sched = \langle \sched_1, \sched_2 \rangle$ with $\sched_i \colon \PiPaths[]{\sg} \rightarrow \Distr(\Act_i)$. We refer to $\sched_i$, $i \in \{ 1, 2 \}$ as a \emph{Player-i policy}. We liberally use $\sched \colon \Paths{\sg} \rightarrow \Distr(\Act)$.
Applying a policy $\sched$ to an SG $\sg$ yields an \emph{induced Markov chain} $\induced{\sg}{\sched} = \langle S', \iota', P' \rangle$ with state space $S' = \Paths{\sg}$, initial state $\iota' = \iota$, and transition function $P'(\pi,\pi')$ defined by $P'(\pi)(\pi \cdot \act s') = \sched(\pi)(\act) \cdot P(\last{\pi},\act)(s')$ and $P'(\pi,\pi') = 0$ otherwise. For any upper bound on the length of the paths, the induced MC is finite. 

In this paper, it is helpful to consider $P$ as being defined using an auxiliary notion of environment actions $\Act_E$, a deterministic environment transition relation $P_{\hat{i}}\colon S_i \times \Act_i \rightarrow A_E$ and a (memoryless, randomized) environment-scheduler $A_E \rightarrow \Distr(S)$.\sj{Move where we need this}

%\paragraph{Properties.}
We consider finite horizon reachability properties.
The probability $\Pr^\mc(\pi)$ of a finite path $\pi$ in a Markov chain $\mc$ is given by the product of the transition probabilities. The probability $\Pr^\mc(\eventually{h} T)$ denotes the probability to reach a state in $T$ with $h$ steps in the induced Markov chain, i.e., $\Pr^\mc(\eventually{h} T)$.
For any SG $\sg$ and scheduler $\sched$, let $\Pr^\sg_\sched(\eventually{h} T)$ denote the probability to reach a state in $T$ with $h$ steps in the induced Markov chain, $\Pr^\sg_\sched(\eventually{h} T)= \Pr^{\induced{\sg}{\sched}}(\eventually{h} T)$.  
\sj{extend this a bit. } 

For any fixed horizon $h$, we can equivalently unfold the SG into its game tree form $\unrolled{\sg}{h}$. $\unrolled{\sg}{h}$ is a SG with 
\sj{add the tree formulation}


\subsection{Control Improvisation}
In control improvisation, we aim to find a policy that satisfy a combination of hard- and soft constraints, and additional generate surprising behaviour. In this paper, we measure the surprise by the causal entropy over the traces in the induced Markov chain. Below, we formalize entropy and then state the formal problem statement. 
\paragraph{Entropy}
Entropy is a measure 

Define entropy on a random variable.\sj{Do}

Define entropy on a Markov chain\sj{Do}


\begin{mdframed}[backgroundcolor=blue!5]
\textbf{The Control Improvisation (CI) Problem}:
Given an SG $\sg$ with target-states $T$ and $G$ and a horizon $h$, does there exists a policy $\sched_1 \in \Sched_1$  such that for every policy $\sched_2 \in \Sched_2$ and with $\sched = \langle \sched_1, \sched_2 \rangle$ it holds that 
\begin{compactenum}
	\item (\emph{hard constraint}) $\Pr^\sg_{\sched}(\eventually{h} T) \geq 1$
	\item (\emph{soft constraint)} $\Pr^\sg_{\sched}(\eventually{h} G) \geq \lambda$
	\item (\emph{randomness constraint}) $H(\sg[\sched]) \geq \kappa$
\end{compactenum}
\end{mdframed}
There is a straightforward extension to a \emph{set of hard constraints}.
Rather than fixing $\kappa$ a priori, we are often interested in limiting the \emph{regret}: The last point then becomes:
$H(\sg[\sched]) \geq (1-\delta) \cdot H(\sg[\sched^{*}])$, where $\sched^{*}$ .... \sj{I am not sure how to define this concisely.} 
Before we continue, we want to establish that Control improvisation problem is a conservative extension of deterministic case as investigated in~\cite{}.
\begin{mdframed}
\textbf{The Deterministic (Reactive) Control Improvisation (DCI) Problem~\cite{}}:
Given a deterministic SG $\sg$ with target-states $T$ and $G$ and a horizon $h$, does there exists a policy $\sched_1 \in \Sched_1$  such that for every policy $\sched_2 \in \Sched_2$ and with $\sched = \langle \sched_1, \sched_2 \rangle$ it holds that 
\begin{compactenum}
	\item (\emph{hard constraint}) $\Pr^\sg_{\sched}(\eventually{h} T) \geq 1$
	\item (\emph{soft constraint)} $\Pr^\sg_{\sched}(\eventually{h} G) \geq \lambda$
	\item (\emph{randomness constraint}) $\Pr(\pi) \leq \delta \text{ for all } \pi \in \Paths[h]{\sg}$.
\end{compactenum}
\end{mdframed}

\begin{theorem}
	For deterministic SGs, the CI and DCI problem coincide.\sj{We may want to defer the proof (sketch) to later}
\end{theorem}





