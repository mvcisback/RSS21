\subsection{Stochastic Games}
%We consider a graph-based game formalism between two players.
A (2.5-player) \emph{stochastic game} (SG) is a tuple $\sg = \langle S,
\iota, \Act, P \rangle$.  The finite set of \emph{states} $S = S_\pOne \cup
S_\pTwo$ is partitioned into a set $S_\pOne$ of (controlled)
$\pOne$-states and a set $S_\pTwo$ of (uncontrolled)
$\pTwo$-states. $\iota \in S_\pOne$ is the \emph{initial state}, $\Act$~is a
finite set of \emph{actions}, and $P\colon S \times \Act \rightarrow
\Distr(S)$ is the \emph{transition function}. For simplicity of
exposition, we assume w.l.o.g. that controlled and uncontrolled
states alternate. Thus, $P$ is defined by two \emph{partial} transition functions:
$P_\pOne\colon S_\pOne \times \Act \rightarrow \Distr(S_\pTwo)$,
$P_\pTwo\colon S_\pTwo \times \Act \rightarrow \Distr(S_\pOne)$. We
identify the available actions\footnote{We use a partial function as we explicitly allow modeling 
unavailable actions, e.g., we can model that a door can only be opened
when close enough to the door.} as $\EnAct(s) \eqdef \{ \act \mid
P(s,\act) \neq \bot \}$. States without available actions, i.e.,
states with $\EnAct(s) = \emptyset$ are called \emph{terminal states}.
The \emph{successor
states} of a state~$s$ and an (enabled) action~$\act$ is the set of
states that are reached from $s$ within one step with a positive
transition probability, i.e., $\Succ(s,\act) \eqdef \{ s' \mid
P(s,\act)(s')>0 \}$, and $\Succ(s) \eqdef\bigcup_{\act \in \EnAct(s)}
\Succ(s,\act)$.


\begin{figure}
\centering
\begin{tikzpicture}
	\node[sstate,initial,initial text=] (s0) {$s_0$};
	\node[astate,right=of s0] (s1) {$s_1$};
	\node[astate,below=of s0] (s2) {$s_2$};
	
	\node[sstate,below=of s1] (s3) {$s_3$};
	
	\node[tstate,right=1.4cm of s1] (target) {$\target$};
	\node[tstate,right=1.4cm of s3] (sink) {$\sink$};
	
	\node[actnode,right=4mm of s1] (a1) {};
	\node[actnode,right=4mm of s3] (a2a) {};
	
	
	\draw[->] (s0) -- node[actnode] {} 
	                  node[near start,auto,elab] {$a$} (s1);
	\draw[->] (s0) -- node[actnode] {}
					  node[near start,elab,right] {$b$} (s2);
					  
	\draw[->] (s2) -- node[actnode] {}
					  node[near start,elab,above] {$a$} (s3);				  
					  
	\draw[->] (s1) -- node[actnode] {}
					  node[near start,elab] {$b$}  (s3);
	\draw[->] (s1) -- node[elab] {$a$} (a1);
	\draw[->] (a1) -- node[elab] {$\nicefrac{1}{3}$} (target);
	\draw[->] (a1) -- node[elab,near start,left] {$\nicefrac{2}{3}$} (sink);
	\draw[->] (a2a) -- node[elab,near start,xshift=0.1cm] {$\nicefrac{1}{3}$} (target);
	\draw[->] (a2a) -- node[elab,pos=0.6,above] {$\nicefrac{2}{3}$} (sink);
	\draw[->] (s3) -- node[elab] {$a$} (a2a);
	\draw[->] (s3) edge[bend right=30] node[actnode] {}
										node[near start,below] {$b$} (sink);
	
	
\end{tikzpicture}	
\caption{A running example.}
\label{fig:toysg}
\end{figure}
\begin{example}
  We introduce a six-state toy-example~(Fig.~\ref{fig:toysg}) to
  illustrate the definitions. Terminal states are drawn with a rectangle,
  $\pOne$-states with a circle and $\pTwo$-states with a diamond. For
  every state $s$ and action $\act$, we draw transitions in the form
  of edges that connect all successors $s'$, and label them with the
  associated probabilities $P(s,\act)(s')$. For conciseness, we omit
  labelling probability $1$ transitions.
\end{example}
	

SGs capture a variety of models.  For example, if $|\EnAct(s)| = 1$ for all uncontrolled states, $s \in S_\pTwo$, then $\sg$ is a
\emph{Markov decision process} (MDP).  If $|\EnAct(s)| = 1 $ for all $s \in S$, then $\sg$ is a \emph{Markov chain}. If
$P(s,\act)$ is a Dirac distribution for every $s \in S$ and $\act \in
\Act$, then $\sg$ is called \emph{deterministic} or a \emph{2-player
game}.

\subsection{Paths and Path Properties}
A finite \emph{path}, $\path$, of length $n$ is a sequence $s_0
\xrightarrow{\act_0} s_1 \xrightarrow{\act_1} s_2 \rightarrow \hdots
\rightarrow s_n$ in $\left( S \times \Act \right)^{n} \times S$ where
$P(s_i,\act_i)(s_{i+1}) > 0$ for each $i$.  We denote the length with
$|\path|$, and denote $s_n$, i.e., the last element of $\path$, with
$\last{\path}$. Further, note that $\pOne$ states are even
indexed and $\pTwo$ states are odd indexed as we assume alternation.
A path, $\path' = s'_0 \xrightarrow{\act'_0} \hdots$, is a
\emph{prefix} of~$\path$, if for all $i \leq |\path'|$, $s_i = s'_i$
and for all $i < |\path'|$, $\act_i = \act'_i$.  The set of all finite
paths of length $n$ is denoted $\Paths[n]{\sg}$, and $\Paths{\sg} =
\bigcup_{n \in \NN} \Paths[n]{\sg}$. We omit $\sg$ whenever it is
 clear from the context.
It is helpful to partition paths
based on their last state: $\POnePaths[]{} = \{ \path \in
 \Paths[]{} \mid \last{\path} \in S_\pOne \}$ and $\PTwoPaths[]{} =
\Paths[]{} \setminus \POnePaths[]{}$.



\begin{example}
  In Fig.~\ref{fig:toysg}, there are two paths that end in $s_3$, $s_0 \xrightarrow{a} s_1 \xrightarrow{b} s_3$ and $s_0 \xrightarrow{b} s_2 \xrightarrow{a} s_3$, both of length $2$. Both paths are in $\POnePaths[]{}$, as $s_3 \in S_\pOne$.
\end{example}
%\paragraph{Policies.} 
Whenever some state $s$ is reached, the corresponding player draws an action from $\EnAct(s)$. As standard, we capture this with the notion of a scheduler\footnote{Also known as \emph{strategy} or \emph{policy}.}.
A \emph{scheduler} is a tuple of \emph{player policies} $\sched = \langle \sched_\pOne, \sched_\pTwo \rangle$
with $\sched_{\player} \colon \PlayerPaths[]{} \rightarrow \Distr(\Act)$ such that $\supp(\sched_i(\path)) \subseteq \EnAct(\last{\path})$ for each $\path$, i.e., for every history, the policy sets a distribution over the enabled successor actions.
For a given path, $\path$ and a policy $\sched_{\player}$, we denote by $\sched_{\player}(\act~|~\path)$ the distribution of actions induced by $\sched_{\player}$ given the path $\path$.
%We refer to $\sched_i$, $i \in \{ 1, 2 \}$ as a \emph{Player-i policy}. 
%We denote the $\pOne$-policy $\pOneSched$ and the $\pTwo$-policy $\pTwoSched$ with $\sched_\pOne$ and $\sched_\pTwo$, respectively.
To ease notation, we liberally use the notation $\sched \colon \Paths{} \rightarrow \Distr(\Act)$, this function is given dependent on which player owns the last state.
 
%
%Applying a policy $\sched$ to an SG $\sg$ yields an \emph{induced Markov chain} $\induced{\sg}{\sched} = \langle S', \iota', P' \rangle$ with state space $S' = \Paths{\sg}$, initial state $\iota' = \iota$, and transition function $P'(\path,\path')$ defined by $P'(\path)(\path \cdot \act s') = \sched(\path)(\act) \cdot P(\last{\path},\act)(s')$ and $P'(\path,\path') = 0$ otherwise. For any upper bound on the length of the paths, the induced MC is finite. 


\begin{example}
  An example for a $\pOne$-policy $\pOneSched$ is given
  by,
  \begin{equation*}
    \pOneSched(\alpha~|~\xi) =
    \begin{cases}
      \nicefrac{1}{2} & \text{if } \alpha \in \{ a,b \}, ~\xi = s_0,\\
      1 & \text{if } \alpha = a,~\xi = s_0\xrightarrow{b}s_2 \xrightarrow{a} s_3,\\
      1 & \text{if } \alpha = b,~\xi = s_0\xrightarrow{a}s_1\xrightarrow{b}s_3.\\
    \end{cases}
  \end{equation*}
\end{example}


%\paragraph{Properties.}
The probability $\Pr(\path \mid \sched)$ of a finite path $\path$ in an SG $\sg$ conditioned on a policy $\sched$ is given by the product of the transition probabilities along a path. 
More precisely, we define the probability $\Pr(\path \mid \sched)$ recursively as:
\begin{equation}
  \begin{split}
    \Pr(s \mid \sched) &\eqdef 1\\
    \Pr(\path~|~\sigma) \eqdef \Pr(\path'\mid\sched)\cdot \sched(&\act\mid\path') \cdot P(\last{\path'},\act)(s')
  \end{split}
\end{equation}
where $\path =  \path' \xrightarrow{\act} s'$.
The probability of a prefix-free set $X \subseteq \Paths{}$  of paths is the sum over the individual path probabilities, $\Pr(X \mid \sched) = \sum_{\path \in X} \Pr(\path \mid \sched )$.

Next, we develop machinery to distinguish between desirable and
undesirable paths. We focus on finite path properties,
referred to as specifications or constraints, that are decidable
within some fixed $\tau \in \Nat$ time steps, e.g., ``Recharge before
t=20.'' Technically, we represent these path properties as prefix free
sets of finite paths, $\soft$, reflecting some formal
property\footnotemark. An example are all paths that end in a particular terminal state~$\target$ within $\tau$ steps.
%In all definitions, we omit the superscript $\sg$ whenever it is clear from the context.

\footnotetext{{Such paths may e.g. be defined using temporal properties such as linear temporal logic over finite traces (LTLf)~\cite{DBLP:conf/ijcai/GiacomoV13}.}}

\subsection{Control Improvisation}
In control improvisation, we aim to find a $\pOne$-policy,
$\pOneSched$, that satisfies a combination of hard- and soft
constraints, and additionally generates surprising behavior, where we
measure the expected surprise by the causal
entropy~\cite{DirectedInfoTheoery} over the paths.

We first define causal entropy on arbitrary sequences of random variables.
Let $\rv{X}_{1:i} \eqdef \rv{X}_1, \hdots, \rv{X}_i$ and $\rv{Y}_{1:i} \eqdef
\rv{Y}_1,\hdots,\rv{Y}_i$ denote two sequences of random variables. The
probability of $ \rv{X}_{1:i}$ causally conditioned on $\rv{Y}_{1:i}$ is:
\begin{equation}
  \causalprob{\rv{X}_{1:i}}{\rv{Y}_{1:i}} \eqdef \prod_{j=1}^i \Pr(\rv{X}_j \mid \rv{X}_{1:j-1}\rv{Y}_{1:j}).
\end{equation}
The causal entropy of $\rv{X}_{1:i}$ given $\rv{Y}_{1:i}$ is then defined as,
\begin{equation}
  H(\rv{X}_{1:i}\mid\mid \rv{Y}_{1:i}) \eqdef \expOver{\rv{X}_{1:i},\rv{Y}_{1:i}}{-\log(\causalprob{\rv{X}_{1:i}}{\rv{Y}_{1:i}})}
\end{equation}
Using the chain rule, one can relate causal entropy to (non-causal) entropy, $H(\rv{X} | \rv{Y}) \eqdef \expOver{\rv{X}}{-\log(\Pr(\rv{X}~|~\rv{Y}))}$ via:
\begin{equation}
  H(\rv{X}_{1:i}\mid\mid \rv{Y}_{1:i}) = \sum_{t=1}^i H(\rv{X}_t \mid \rv{Y}_{1:t}, X_{1:t-1})
\end{equation}
This relation shows that:
(1)~Causal entropy is always lower bounded by non-causal entropy (and
thus non-negative). 
(2)~Causal entropy can be computed ``backward in time''.
(3)~Causal and non-causal conditioning can be mixed,

\vspace{-1.1em}
\begin{equation}
  H(\rv{X}_{1:i}\mid\mid \rv{Y}_{1:i} \mid Z) \eqdef \sum_{t=1}^i H(\rv{X}_t \mid \rv{Y}_{1:t}, X_{1:t-1}, Z).
\end{equation}
    
Intuitively, and contrary to non-causal entropy, causal entropy does \emph{not}
condition on variables that have not been revealed, e.g., on events in the future. 
This makes causal entropy particularly well suited for measuring
predictability in \emph{sequential} decision making problems, as the 
agents cannot observe the future~\cite{mceThesis}.

We now define causal entropy in stochastic games.
To that end, recall that a path alternates states and actions.  The
next state after observing a sequence of state-action pairs is a
random variable. Formally, given $\sg$ and a scheduler 
$\sched$, let us denote by $\rv{A}^{\pOne}_{1:i}$ and
$\rv{S}_{1:i}$ random variable sequences for
$\pOne$-player actions and states respectively. The causal entropy of
controllable actions in $\tau$-length paths under $\sched$ is then,
\begin{equation}
  H_\tau(\sigma) \eqdef H( \rv{A}^{\pOne}_{1:\tau'} \mid\mid \rv{S}_{1:\tau} ),
\end{equation}
where $\tau' = \lceil \frac{\tau}{2}\rceil$ is the number of $\pOne$-actions due to alternation.

%, which we liberally use the set $\Paths[\tau]{\sg \mid \sched}$ to indicate these sequences (and the distribution over them) .  We want to express the causal entropy of a sequence of $\pOne$-action choices given the paths. 
% For that, let us define $\Act_\pOne(\path)$ as the sequence $\act_{j_1} \act_{j_2} \hdots \act_{j_n}$ from $\path = s_0\act_0\hdots s_n$ where $j_i$ are the indices such that $s_{j_i} \in S_\pOne$, i.e., 
% $\Act_\pOne(\path)$ denotes the sequence of action choices of $\pOne$. We can lift the operation to sets: $\Act_\pOne(X) = \{ \Act_\pOne(\path) \mid \path \in X \}$, which we also use to denote the associated random variables.
%\[ H^\sg_\tau(\sigma) \colonequals H( \Act_\pOne(\Paths[\tau]{\sg \mid \sched})   \mid\mid \Paths[\tau]{\sg \mid \sched} ).  \]


\begin{example}
  Consider the uniform $\pOne$ policy on Fig.~\ref{fig:toysg}. If $\sched_{\pTwo}(a\mid \xi) = 1$. $H_\tau(\sigma) = \log(2) + \nicefrac{1}{2}(\log(2))$. Note, only $\pOne$ can \emph{add} entropy, while $\pTwo$ and stochastic transitions yield convex combinations via expectation.
\end{example}

\noindent
We now formalize the problem statement. 
\begin{mdframed}[backgroundcolor=blue!5,nobreak=true]
\textbf{The Entropic Control Improvisation (ERCI) Problem}:
Given a SG $\sg$, $\tau$-bounded path properties $\hard$ and $\soft$, and thresholds $\scthreshold \in [0,1]$ and $\randomness \in [0,\infty)$, find a $\pOne$-policy $\pOneSched$ (or report that none exists) such that for every $\pTwo$-policy $\pTwoSched$,
\begin{compactenum}
	\item (\emph{hard constraint}) $\Pr(\hard \mid \sched) \geq 1$
	\item (\emph{soft constraint)} $\Pr(\soft \mid \sched) \geq \scthreshold$
\item (\emph{randomness constraint}) $H_\tau(\sigma) \geq \randomness$
\end{compactenum}
where  $\sched = \langle \pOneSched, \pTwoSched \rangle$.
\end{mdframed}
We say that an instance of the ERCI problem is realizable, if an appropriate $\pOneSched$ exists and call such $\pOneSched$ an \emph{improviser}. The problem is unrealizable otherwise.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
