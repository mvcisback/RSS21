\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (author)
   /Title  (title)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots)
}

\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{paralist}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{colonequals}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}


\usetikzlibrary{patterns,arrows,backgrounds,calc,shapes,shadows,decorations.pathmorphing,decorations.pathreplacing,automata,shapes.multipart,positioning,shapes.geometric,fit,circuits,trees,shapes.gates.logic.US,fit,decorations.markings}

\tikzset{sstate/.style={circle, draw=black, inner sep=1pt}}

\makeatletter
\let\MYcaption\@makecaption
\makeatother

\usepackage[font=footnotesize]{subcaption}

\makeatletter
\let\@makecaption\MYcaption
\makeatother

\newcommand{\NN}{\mathbb{N}}
\newcommand{\mc}{\mathcal{D}}
\newcommand{\sg}{\mathcal{G}}
\renewcommand{\path}{\xi}
\newcommand{\eventually}[1]{\lozenge^{\leq #1}}
\newcommand{\sched}{\sigma}
\newcommand{\Sched}{\Sigma}
\newcommand{\pol}{\sched}
\newcommand{\Distr}{\ensuremath{\textsl{Distr}}}
\newcommand{\act}{\alpha}
\newcommand{\Act}{A}
\newcommand{\scp}{p}
\newcommand{\scthreshold}{\mathbf{p}}
\newcommand{\target}{s_{\top}} 
\newcommand{\sink}{s_{\bot}}
\newcommand{\rndp}{h}
\newcommand{\randomness}{\mathbf{h}}
\newcommand{\last}[1]{{#1}_\downarrow}
\newcommand{\pOneSched}{\mathbf{ego}}
\newcommand{\POneScheds}{\Sigma_\pOne}
\newcommand{\PTwoScheds}{\Sigma_\pTwo}
\newcommand{\pTwoSched}{\mathbf{env}}
\newcommand{\pOne}{\mathsf{ego}}
\newcommand{\pTwo}{\mathsf{env}}
\newcommand{\horizon}{\tau}
\newcommand{\solutions}{\mathbb{S}}
\newcommand{\solfuncp}{f_\mathbb{S}}
\newcommand{\scopt}{\scp^{*}}
\newcommand{\rndopt}{\rndp^{*}}

\newcommand{\scmin}{\scp^{-}}
\newcommand{\rndmin}{\rndp^{-}}
\newcommand{\pathslbl}{\Xi}
\newcommand{\Paths}[2][]{\pathslbl^{#2}_{#1}}
\newcommand{\POnePaths}[2][]{{\pathslbl^{#2}_{#1}}/_{\downarrow_{1}}}
\newcommand{\PTwoPaths}[2][]{{\pathslbl^{#2}_{#1}}/_{\downarrow_{2}}}
\newcommand{\PiPaths}[2][]{{\pathslbl^{#2}_{#1}}/_{\downarrow_{i}}}
\newcommand{\unrolled}[2]{\textsf{Tree}(#1,#2)}
\newcommand{\induced}[2]{#1[#2]}
\newcommand{\causalprob}[2]{\Pr(#1\mid\mid#2)}
\newcommand{\expOver}[2]{\mathbb{E}_{#1}[#2]}



\setlength\marginparwidth{110pt}
\newcommand{\colorpar}[3]{\colorbox{#1}{\parbox{#2}{#3}}}
\newcommand{\marginremark}[3]{\marginpar{\colorpar{#2}{\linewidth}{\color{#1}#3}}}
\newcommand{\commentside}[2]{\marginpar{\color{#1}\tiny#2}}
\newcommand{\TODO}[1]{\commentside{teal}{\textsc{Todo:} #1}}
\newcommand{\REMARK}[1]{\commentside{teal}{\textsc{Remark:} #1}}\newcommand{\sj}[1]{\marginremark{black}{red!10!white}{\scriptsize{[SJ]~ #1}}}
\newcommand{\mvc}[1]{\marginremark{black}{gray!10!white}{\scriptsize{[MVC]~ #1}}}


%\institute{University of California, Berkeley, CA, USA}


\begin{document}

% paper title
\title{Entropy-Guided Control Improvisation}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
Declarative constraints are a powerful tool to define high-level controllers. 
However, most algorithms that synthesise controllers from these constraints construct deterministic policies -- which limits robustness and unpredictability.  
Control improvisation aims to overcome these weaknesses. Key are three types of constraints: Hard constraints describe behavior that must occur, 
soft constraints describe behavior that typically occurs, and a randomization constraint ensures variance in the behavior. 
We provide the first control improvisation framework, based on causal entropy to describe randomization, that supports to adverserial and probabilistic environments. These features allow us to synthesise powerful robust policies that generatate unpredictable behavior. 

\end{abstract}

\IEEEpeerreviewmaketitle

%\maketitle\sj{Daniel?}
%\begin{abstract}
%	Efficacious controller synthesis is a key ingredient in the design and analysis of complex systems. We study the design of controllers that have a high entropy, that is, whose behavior or nature is surprising. The synthesis of such controllers is key in domains like testing and security. 
%	In particular, our paper studies control improvisation and compares them with randomly sampling adequate policies. The only difference in obtained policies is in their notion of entropy, but the problems are significantly different.  We illustrate and contrast their merits and limitations. Furthermore, we provide algorithms that solve both control improvisation problems. Prominently, we solve the control improvisation problem for Markov decision processes by relating it to recent results from inference from demonstrations, and then extend this approach to stochastic games. We present a prototypical implementation that efficiently solves controller synthesis problems from the security and testing domain. 
%\end{abstract}
\section{Introduction}
\input{introduction}
\section{Motivating Example}
\input{example}
\section{Problem Statement}
This section formalizes the problem statement. We show that our problem statement is a conservative extension of the existing reactive control improvisation framework and we reformulate our problem statement in terms of finding a Pareto-curve. We start with some necessary definitions and notations.


\input{problem}




\section{The Control Improvisation Problem for MDPs}
\label{sec:mdps}
\color{black!50}
We consider $P$ as being defined using an auxiliary notion of environment actions $\Act_E$, a deterministic environment transition relation $P_{\hat{i}}\colon S_i \times \Act_i \rightarrow A_E$ and a (memoryless, randomized) environment-scheduler $A_E \rightarrow \Distr(S)$.



It is convenient to talk about a third player, $\mathsf{rnd}$, that owns all states with $|\Act|$\sj{needs available actions}


We present an algorithm for the control improvisation problem for MDPs. The key insight is to reuse ideas from policy inference from specifications~\cite{DBLP:conf/cav/Vazquez-Chanlatte20}.
In the next section, we extend this idea to SGs. 




\subsection{Soft constraint vs randomness}
\paragraph{Max randomness without soft constraint}
The uniform random policy maximises randomness.
\begin{remark}
Notice that here, it is important that our induced Markov chain contains the state choices as well.	
\end{remark}


\paragraph{Monotonicity}
\sj{add lemma}
We can (conceptually) sort all policies by their induced reachability to reach states $G$, i.e., by the (quantitative) satisfaction of the soft constraint. For each policy, we can plot the associated maximal entropy, $H^*(\lambda) = \sup_{\sched} \{ H(\sg[\sched]) \mid \Pr_{\sched}(\eventually{h} G) = \lambda\}$, see Fig.~\ref{fig:entropyvspsat}.
The policy that maximises the entropy is (typically) somewhere in the middle--and the function is monotonically decreasing on both sides. 
In this paper, we are only interested in the right side of the graph:
We first construct the max-entropy policy $\sched^*$. If the induced probability $\Pr_{\sched^*}(\eventually{h} G) \geq \lambda$ and the associated entropy $H(\sg[\sched^*]) \geq \kappa$, then we found a solution to the CI problem. If $\Pr_{\sched^*}(\eventually{h} G) \geq \lambda$ but $H(\sg[\sched^*]) \geq \kappa$, then the CI problem is unsatisfiable (recall that $\sched^*$ maximizes entropy).
The remaining case is that $\lambda \geq \Pr_{\sched^*}(\eventually{h} G)$, i.e., the right side of the graph.
For that, it holds that the higher $\lambda$, the lower $\sup_{\sched} \{ H(\sg[\sched]) \mid \Pr_{\sched}(\eventually{h} G) = \lambda\}$~\sj{Can we cite something, do we need to prove this ourselves?}. 
\begin{figure}
\begin{tikzpicture}[scale=3]
	 \draw[->] (-0.05, 0) -- (1, 0) node[below]{$\Pr(X_{\lozenge\mathbf{\top}} \mid \sched)$};
  	\draw[->] (0, -0.05) -- (0, 0.8) node[above] {$H(\sched)$};
  	\draw[-,dashed] (0.5,0) -- (0.5,0.5184);
  	\draw[-,dashed] (0.0,0.5184) -- (0.5,0.5184);
  \draw[ domain=0:1, smooth, variable=\x, blue] plot ({\x}, {8*(1-\x)*(1-\x)*\x*\x});
\end{tikzpicture}	
\caption{Entropy vs reachability probability for the soft constraint}
\label{fig:entropyvspsat}
\end{figure}
\sj{make convex}

Result: A computation tree, computing the max entropy over schedulers that achieve the goal with prob exactly $\lambda$.
A variant in which we explore the trade-off between soft constraint and entropy is a simple extension that employs a binary search.
\subsection{Entropy maximization for a fixed reachability probability}

An optimal policy follows the Bellman equation
\[ V(s,h)  = \max_{\act \in \Act} \sum_{s'} P(s,\act,s') \cdot V(s',h-1),\]
where $V$ denotes the value, which here corresponds to the $h$-step reachability probability from state $s$ onwards.
In particular, in every step, we take the actions that maximize the probability to reach the target (within the horizon). 
If we now want to reduce the probability to reach the goal (and thereby allow to increase the entropy) we can select in every step (with some probability) one or more of the suboptimal actions. We call this possibility \emph{wiggle room}. 
\paragraph{How to wiggle?}
We should pick suboptimal actions in such a way that we maximize the overall entropy. Let us therefore generalize the Bellman equation:

We observe that for $\theta \rightarrow \infty$, this equation matches the (traditional) Bellman equation. 
Furthermore, for $\theta \rightarrow 0$, this equation yields a policy that maximises entropy.
Thus, intuitively, we can span the whole (right part of) the graph as depicted in Fig.~\ref{fig:entropyvspsat}.

\sj{Here we need to add some more formal argument}

\paragraph{An Algorithm for control improvisation}

We use binary search (more precisely: root finding of a monotonic function) to select a 

The algorithm 

\sj{Do we know the complexity of the decision problem? To encode it in a set of constraints we need extended polynomial equations (with exponents, that is), right?}



 
\color{black}
\section{The Control Improvisation Problem for SGs}
\label{sec:sgs}
\paragraph{Maximising entropy against an adverserial player}

\subsection{Playing against deterministic adverseries}
To formalise our argument
\paragraph{Reformulation as a set of MDPs}

\paragraph{}


\subsection{Randomizing adversaries will not play better}

\subsection{Non-observable adversaries}
%\begin{mdframed}
%
%\begin{compactenum}
%	\item $\Pr^\sg_{\langle \sched_1,\sched_2 \rangle}(\eventually{h} T) \geq 1$
%	\item $\Pr^\sg_{\langle \sched_1,\sched_2 \rangle}(\eventually{h} G) \geq \lambda$ 
%\end{compactenum}
%\end{mdframed}
%\sj{Define randomly selected policy}
%\sj{Describe in terms of pMDPp}


\section{Implementation and Empirical Evaluation}

\section{Related work}
\color{red}
\cite{DBLP:journals/corr/abs-2009-10883}
\cite{DBLP:journals/jcss/BrazdilCFK17}

Add related work from control improv.
Add related work with entropy.
Maybe briefly discuss synthesis algorithms mentioned at start of intro?
\color{black}



Path-finding has long been considered a multi-objective problem itself~\cite{DBLP:conf/icra/AmigoniG05,DBLP:journals/eswa/NazarahariKD19,DBLP:conf/icml/XuTMRSM20}.
These works differ prominently in two aspects: they do not trade-off randomization and performance, and they do not trade-off declarative and formal constraints with the accompanying formal guarentees, but are more search-based. 
Finding policies that optimize reward objectives is well-studied in the field of reinforcement learning, and has been extended to generate Pareto-fronts for multiple objectives~\cite{DBLP:conf/icml/NatarajanT05,DBLP:conf/adprl/ParisiPSBR14}.

Synthesis in MDPs with multiple hard and soft constraints (often over indefinite horizons) is a well-studied problem~\cite{DBLP:conf/stacs/ChatterjeeMH06,DBLP:conf/tacas/EtessamiKVY07,DBLP:conf/atva/ForejtKP12,DBLP:journals/fmsd/RandourRS17}.  In this setting, one generates deterministic policies and their convex combinations. Put differently, randomization is \emph{not an objective}, but rather a consequence. Interestingly, in \cite{DBLP:conf/tacas/DelgrangeKQR20} one even argues for the \emph{absence} of randomization in various domains.  
The original results sparked interest in different extension to MDPs and the type of soft constraints, such as continuous MDPs \cite{DBLP:journals/csysl/HaesaertNS21} and continuous-time MDPs~\cite{DBLP:conf/cav/QuatmannJK17},  cost-bounded reachability \cite{DBLP:journals/jar/HartmannsJKQ20}, or mean-payoff properties~\cite{DBLP:journals/corr/abs-1104-3489}. 
The algorithms have also been extended towards stochastic games~\cite{DBLP:conf/mfcs/ChenFKSW13,DBLP:journals/sttt/KwiatkowskaPW18}.
Finally, notions of lexicographic multi-objective synthesis~\cite{DBLP:conf/cav/ChatterjeeKWW20} -- in which one optimizes a secondary criterion among all policies that are optimal with respect to a first criterion bare some resemblance with the algorithm we consider. 
These algorithms have been put in a robotics context in~\cite{DBLP:journals/ijrr/LacerdaFPH19}.


\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}
