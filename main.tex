\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (author)
   /Title  (title)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots)
}

\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{paralist}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{colonequals}
\newtheorem{theorem}{Theorem}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\usetikzlibrary{patterns,arrows,backgrounds,calc,shapes,shadows,decorations.pathmorphing,decorations.pathreplacing,automata,shapes.multipart,positioning,shapes.geometric,fit,circuits,trees,shapes.gates.logic.US,fit,decorations.markings}

\tikzset{sstate/.style={circle, draw=black, inner sep=1pt}}


\newcommand{\NN}{\mathbb{N}}
\newcommand{\mc}{\mathcal{D}}
\newcommand{\sg}{\mathcal{G}}
\renewcommand{\path}{\xi}
\newcommand{\eventually}[1]{\lozenge^{\leq #1}}
\newcommand{\sched}{\sigma}
\newcommand{\Sched}{\Sigma}
\newcommand{\pol}{\sched}
\newcommand{\Distr}{\ensuremath{\textsl{Distr}}}
\newcommand{\act}{\alpha}
\newcommand{\Act}{A}
\newcommand{\scthreshold}{p}
\newcommand{\randomness}{h}
\newcommand{\last}[1]{{#1}_\downarrow}
\newcommand{\pOneSched}{\mathbf{ego}}
\newcommand{\POneScheds}{\Sigma_\pOne}
\newcommand{\PTwoScheds}{\Sigma_\pTwo}
\newcommand{\pTwoSched}{\mathbf{env}}
\newcommand{\pOne}{\mathsf{ego}}
\newcommand{\pTwo}{\mathsf{env}}
\newcommand{\horizon}{\tau}
\newcommand{\solutions}{\mathbb{S}}

\newcommand{\pathslbl}{\Xi}
\newcommand{\Paths}[2][]{\pathslbl^{#2}_{#1}}
\newcommand{\POnePaths}[2][]{{\pathslbl^{#2}_{#1}}/_{\downarrow_{1}}}
\newcommand{\PTwoPaths}[2][]{{\pathslbl^{#2}_{#1}}/_{\downarrow_{2}}}
\newcommand{\PiPaths}[2][]{{\pathslbl^{#2}_{#1}}/_{\downarrow_{i}}}
\newcommand{\unrolled}[2]{\textsf{Tree}(#1,#2)}
\newcommand{\induced}[2]{#1[#2]}
\newcommand{\causalprob}[2]{\Pr(#1\mid\mid#2)}
\newcommand{\expOver}[2]{\mathbb{E}_{#1}[#2]}



\setlength\marginparwidth{110pt}
\newcommand{\colorpar}[3]{\colorbox{#1}{\parbox{#2}{#3}}}
\newcommand{\marginremark}[3]{\marginpar{\colorpar{#2}{\linewidth}{\color{#1}#3}}}
\newcommand{\commentside}[2]{\marginpar{\color{#1}\tiny#2}}
\newcommand{\TODO}[1]{\commentside{teal}{\textsc{Todo:} #1}}
\newcommand{\REMARK}[1]{\commentside{teal}{\textsc{Remark:} #1}}\newcommand{\sj}[1]{\marginremark{black}{red!10!white}{\scriptsize{[SJ]~ #1}}}
\newcommand{\mvc}[1]{\marginremark{black}{gray!10!white}{\scriptsize{[MVC]~ #1}}}


%\institute{University of California, Berkeley, CA, USA}


\begin{document}

% paper title
\title{Entropy-Guided Control Improvisation}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
Declarative constraints are a powerful tool to define high-level controllers. 
However, most algorithms that synthesise controllers from these constraints construct deterministic policies -- which limits robustness and unpredictability.  
Control improvisation aims to overcome these weaknesses. Key are three types of constraints: Hard constraints describe behavior that must occur, 
soft constraints describe behavior that typically occurs, and a randomization constraint ensures variance in the behavior. 
We provide the first control improvisation framework, based on causal entropy to describe randomization, that supports to adverserial and probabilistic environments. These features allow us to synthesise powerful robust policies that generatate unpredictable behavior. 

\end{abstract}

\IEEEpeerreviewmaketitle

%\maketitle\sj{Daniel?}
%\begin{abstract}
%	Efficacious controller synthesis is a key ingredient in the design and analysis of complex systems. We study the design of controllers that have a high entropy, that is, whose behavior or nature is surprising. The synthesis of such controllers is key in domains like testing and security. 
%	In particular, our paper studies control improvisation and compares them with randomly sampling adequate policies. The only difference in obtained policies is in their notion of entropy, but the problems are significantly different.  We illustrate and contrast their merits and limitations. Furthermore, we provide algorithms that solve both control improvisation problems. Prominently, we solve the control improvisation problem for Markov decision processes by relating it to recent results from inference from demonstrations, and then extend this approach to stochastic games. We present a prototypical implementation that efficiently solves controller synthesis problems from the security and testing domain. 
%\end{abstract}
\section{Introduction}
\input{introduction}
\section{Motivating Example}
\input{example}
\section{Problem Statement}
This section formalizes the problem statement. We show that our problem statement is a conservative extension of the existing reactive control improvisation framework and we reformulate our problem statement in terms of finding a Pareto-curve. We start with some necessary definitions and notations.


\input{problem}
\section{Control improvisation versus Policy Sampling}
In this section, we discuss the nature of the randomness as imposed by the CI problem, and relate this to improvising policies, i.e., to randomly sampling policies that satisfy the hard and soft constraint.




\begin{mdframed}

\begin{compactenum}
	\item $\Pr^\sg_{\langle \sched_1,\sched_2 \rangle}(\eventually{h} T) \geq 1$
	\item $\Pr^\sg_{\langle \sched_1,\sched_2 \rangle}(\eventually{h} G) \geq \lambda$ 
\end{compactenum}
\end{mdframed}
\sj{Define randomly selected policy}
\sj{Describe in terms of pMDPp}




\section{The Control Improvisation Problem for MDPs}
\color{black!50}
For any fixed horizon $h$, we can equivalently unfold the SG into  $\unrolled{\sg}{h}$. $\unrolled{\sg}{h}$ is a SG with 
\sj{add the tree formulation}
We consider $P$ as being defined using an auxiliary notion of environment actions $\Act_E$, a deterministic environment transition relation $P_{\hat{i}}\colon S_i \times \Act_i \rightarrow A_E$ and a (memoryless, randomized) environment-scheduler $A_E \rightarrow \Distr(S)$.

\color{black}

We present an algorithm for the control improvisation problem for MDPs. The key insight is to reuse ideas from policy inference from specifications~\cite{DBLP:conf/cav/Vazquez-Chanlatte20}.
In the next section, we extend this idea to SGs. 

\paragraph{Preprocessing for hard constraints}
\begin{itemize}
\item We construct the computation tree	
\item We cut all policies that violate the hard constraint, this is easy on the computation tree
\end{itemize}
Result: A computation tree, a soft constraint and a randomness constraint.



\subsection{Soft constraint vs randomness}
\paragraph{Max randomness without soft constraint}
The uniform random policy maximises randomness.
\begin{remark}
Notice that here, it is important that our induced Markov chain contains the state choices as well.	
\end{remark}


\paragraph{Monotonicity}
\sj{add lemma}
We can (conceptually) sort all policies by their induced reachability to reach states $G$, i.e., by the (quantitative) satisfaction of the soft constraint. For each policy, we can plot the associated maximal entropy, $H^*(\lambda) = \sup_{\sched} \{ H(\sg[\sched]) \mid \Pr_{\sched}(\eventually{h} G) = \lambda\}$, see Fig.~\ref{fig:entropyvspsat}.
The policy that maximises the entropy is (typically) somewhere in the middle--and the function is monotonically decreasing on both sides. 
In this paper, we are only interested in the right side of the graph:
We first construct the max-entropy policy $\sched^*$. If the induced probability $\Pr_{\sched^*}(\eventually{h} G) \geq \lambda$ and the associated entropy $H(\sg[\sched^*]) \geq \kappa$, then we found a solution to the CI problem. If $\Pr_{\sched^*}(\eventually{h} G) \geq \lambda$ but $H(\sg[\sched^*]) \geq \kappa$, then the CI problem is unsatisfiable (recall that $\sched^*$ maximizes entropy).
The remaining case is that $\lambda \geq \Pr_{\sched^*}(\eventually{h} G)$, i.e., the right side of the graph.
For that, it holds that the higher $\lambda$, the lower $\sup_{\sched} \{ H(\sg[\sched]) \mid \Pr_{\sched}(\eventually{h} G) = \lambda\}$~\sj{Can we cite something, do we need to prove this ourselves?}. 
\begin{figure}
\begin{tikzpicture}[scale=3]
	 \draw[->] (-0.05, 0) -- (1, 0) node[right]{$\Pr_\sched(\eventually{h}G)$};
  	\draw[->] (0, -0.05) -- (0, 0.8) node[above] {$H^*$};
  	\draw[-,dashed] (0.4,0) -- (0.4,0.5184);
  	\draw[-,dashed] (0.0,0.5184) -- (0.4,0.5184);
  \draw[ domain=0:1, smooth, variable=\x, blue] plot ({\x}, {15*(1-\x)*(1-\x)*(1-\x)*\x*\x});
\end{tikzpicture}	
\label{fig:entropyvspsat}
\caption{Entropy vs reachability probability for the soft constraint}
\end{figure}
Result: A computation tree, computing the max entropy over schedulers that achieve the goal with prob exactly $\lambda$.
A variant in which we explore the trade-off between soft constraint and entropy is a simple extension that employs a binary search.
\subsection{Entropy maximization for a fixed reachability probability}

An optimal policy follows the Bellman equation
\[ V(s,h)  = \max_{\act \in \Act} \sum_{s'} P(s,\act,s') \cdot V(s',h-1),\]
where $V$ denotes the value, which here corresponds to the $h$-step reachability probability from state $s$ onwards.
In particular, in every step, we take the actions that maximize the probability to reach the target (within the horizon). 
If we now want to reduce the probability to reach the goal (and thereby allow to increase the entropy) we can select in every step (with some probability) one or more of the suboptimal actions. We call this possibility \emph{wiggle room}. 
\paragraph{How to wiggle?}
We should pick suboptimal actions in such a way that we maximize the overall entropy. Let us therefore generalize the Bellman equation:

We observe that for $\theta \rightarrow \infty$, this equation matches the (traditional) Bellman equation. 
Furthermore, for $\theta \rightarrow 0$, this equation yields a policy that maximises entropy.
Thus, intuitively, we can span the whole (right part of) the graph as depicted in Fig.~\ref{fig:entropyvspsat}.

\sj{Here we need to add some more formal argument}

\paragraph{An Algorithm for control improvisation}

We use binary search (more precisely: root finding of a monotonic function) to select a 

The algorithm 

\sj{Do we know the complexity of the decision problem? To encode it in a set of constraints we need extended polynomial equations (with exponents, that is), right?}



 

\section{The Control Improvisation Problem for SGs}
\paragraph{Maximising entropy against an adverserial player}

\subsection{Playing against deterministic adverseries}
To formalise our argument
\paragraph{Reformulation as a set of MDPs}

\paragraph{}


\subsection{Randomizing adversaries will not play better}

\subsection{Non-observable adversaries}

\section{Discussion}
Before we continue, we want to establish that Control improvisation problem is a conservative extension of deterministic case as investigated in~\cite{}.
\begin{mdframed}
\textbf{The Deterministic (Reactive) Control Improvisation (DCI) Problem~\cite{}}:
Given a \emph{deterministic} SG $\sg$, finite path sets $X_\psi$ and $X_\varphi$, and a threshold $p \in (0,1)$,  find a $\pOne$-policy $\pOneSched \in \POneScheds$  such that for every $\pTwo$-policy $\pTwoSched \in \PTwoScheds$ 
\begin{compactenum}
\item (\emph{hard constraint}) $\Pr^\sg_{\sched}(X_\psi) \geq 1$
	\item (\emph{soft constraint)} $\Pr^\sg_{\sched}(X_\varphi) \geq \scthreshold$
	\item (\emph{randomness constraint}) $\Pr(\path) \leq \delta \text{ for all } \path \in \Paths[h]{\sg[\sched]}$.
\end{compactenum}
\end{mdframed}

\begin{theorem}
	For deterministic SGs, the ERCI and DCI problem coincide.
\end{theorem}
We defer a discussion and a precise statement to Lemma~\ref{}.

\section{Implementation and Empirical Evaluation}

\section{Related work list}
\cite{DBLP:journals/corr/abs-2009-10883}
\cite{}



\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}
