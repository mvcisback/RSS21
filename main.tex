\documentclass[runningheads]{llncs}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{paralist}
\usepackage{mdframed}
\usepackage{xcolor}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\mc}{\mathcal{D}}
\newcommand{\sg}{\mathcal{G}}
\newcommand{\eventually}[1]{\lozenge^{\leq #1}}
\newcommand{\sched}{\sigma}
\newcommand{\Sched}{\Sigma}
\newcommand{\pol}{\sched}
\newcommand{\Distr}{\ensuremath{\textsl{Distr}}}
\newcommand{\act}{\alpha}
\newcommand{\Act}{A}
\newcommand{\last}[1]{{#1}_\downarrow}
\newcommand{\Paths}[2][]{\Pi^{#2}_{#1}}
\newcommand{\POnePaths}[2][]{\Pi^{\downarrow_{1}#2}_{#1}}
\newcommand{\PTwoPaths}[2][]{\Pi^{\downarrow_{2}#2}_{#1}}
\newcommand{\PiPaths}[2][]{\Pi^{\downarrow_{i}#2}_{#1}}
\newcommand{\unrolled}[2]{\textsf{Tree}(#1,#2)}
\newcommand{\induced}[2]{#1[#2]}


\setlength\marginparwidth{110pt}
\newcommand{\colorpar}[3]{\colorbox{#1}{\parbox{#2}{#3}}}
\newcommand{\marginremark}[3]{\marginpar{\colorpar{#2}{\linewidth}{\color{#1}#3}}}
\newcommand{\commentside}[2]{\marginpar{\color{#1}\tiny#2}}
\newcommand{\TODO}[1]{\commentside{teal}{\textsc{Todo:} #1}}
\newcommand{\REMARK}[1]{\commentside{teal}{\textsc{Remark:} #1}}\newcommand{\sj}[1]{\marginremark{black}{red!10!white}{\scriptsize{[SJ]~ #1}}}
\newcommand{\mvc}[1]{\marginremark{black}{gray!10!white}{\scriptsize{[MVC]~ #1}}}

\title{On Control Improvisation for Stochastic Games}
\author{Marcell Vazquez-Chanlatte \and Sebastian Junges \and Sanjit A.\ Seshia}
\institute{University of California, Berkeley, CA, USA}

\begin{document}
\maketitle\sj{Daniel?}
\begin{abstract}
	Efficacious controller synthesis is a key ingredient in the design and analysis of complex systems. We study the design of controllers that have a high entropy, that is, whose behavior or nature is surprising. The synthesis of such controllers is key in domains like testing and security. 
	In particular, our paper studies control improvisation and compares them with randomly sampling adequate policies. The only difference in obtained policies is in their notion of entropy, but the problems are significantly different.  We illustrate and contrast their merits and limitations. Furthermore, we provide algorithms that solve both control improvisation problems. Prominently, we solve the control improvisation problem for Markov decision processes by relating it to recent results from inference from demonstrations, and then extend this approach to stochastic games. We present a prototypical implementation that efficiently solves controller synthesis problems from the security and testing domain. 
\end{abstract}
\section{Introduction}
\input{introduction}
\section{Problem Statement}
\input{problem}
\section{Maximum Entropy Traces and Policy Improvisation}
In this section, we discuss the nature of the randomness as imposed by the CI problem, and relate this to improvising policies, i.e., to randomly sampling policies that satisfy the hard and soft constraint.

\begin{mdframed}

\begin{compactenum}
	\item $\Pr^\sg_{\langle \sched_1,\sched_2 \rangle}(\eventually{h} T) \geq 1$
	\item $\Pr^\sg_{\langle \sched_1,\sched_2 \rangle}(\eventually{h} G) \geq \lambda$ 
\end{compactenum}
\end{mdframed}
\sj{Define randomly selected policy}




\section{Solving the Control Improvisation Problem for MDPs}
We present an algorithm for the control improvisation problem for MDPs. The key insight is to reuse ideas from policy inference from specifications~\cite{}. In the next section, we extend this idea to SGs.

\section{Solving the Control Improvisation Problem for SGs}

\section{Empirical Evaluation}

\end{document}
